<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Segundamano's Backstage]]></title>
  <link href="http://backstage.segundamano.mx/atom.xml" rel="self"/>
  <link href="http://backstage.segundamano.mx/"/>
  <updated>2015-06-29T16:04:18-05:00</updated>
  <id>http://backstage.segundamano.mx/</id>
  <author>
    <name><![CDATA[Segundamano.mx Tech Team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[FreeIPA for Your Infrastructure Daily Operations]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/06/29/freeipa-control-access-and-okta/"/>
    <updated>2015-06-29T15:42:23-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/06/29/freeipa-control-access-and-okta</id>
    <content type="html"><![CDATA[<p>When you manage lots of servers (Unix and Linux like), dozens or hundreds of them, one thing for sure that you must take in account is the Access Control, and there are many many tools to help us to that specific task, but let&rsquo;s be realists!  In a minimal decent infrastructure implementation you don?t need to have only Access Control, you need as well policies depending on the user&rsquo;s profile. For example, you are not going to provide root permissions to a sales guy in a linux server for making queries to the main database or if you just want certain developers can execute a critical script (bash or python or whatever).</p>

<p>And we can say, come on! I have a tool that allows me to define Access Control as well as robust policies rules (<em>all mention before know as Identity and Policies</em>), for example LDAP or the well known MS Active Directory. But the Access Control is the only thing we have to take care about in our daily minimal server&rsquo;s operation?, obviously No! What about synchronization? this is to have all our server in the correct time for having accurate logs or naming resolution, if we want a friendly way to identify our servers, you know, things that are essentials in order to ensure that the basic communication and security its guaranteed.</p>

<p>Let&rsquo;s stop to talk about needs that aren?t new for us and let&rsquo;s talk about the solutions or in this case Segundamano&rsquo;s solution.</p>

<h2><strong>FreeIPA</strong></h2>

<p>We are not going to provide or copy/paste an explanation about what FreeIPA is from Wikipedia o Freeipa.org, in that case, please feel free to do it by yourself. We prefer to tell you in our experience what FreeIPA is doing for us. In Segundamano we use FreeIPA for the following approaches:</p>

<ul>
<li><strong>DNS Resolution</strong></li>
</ul>


<p>FreeIPA offers an integration of many open source components like Bind DNS Server, so we decided to use it as our main DNS Platform, In this way we can have multiple Bind slaves servers from it.</p>

<p>And why is this useful? Well It is useful because we want to have an instance for FreeIPA in each environment or Data Center in order to accelerate the DNS resolution and offer high availability.</p>

<p>Maybe you are wondering about what is a FreeIPA instance? Ok this topic will be explained on a next section on this document when we talk about replication which is a super nice feature.</p>

<ul>
<li><strong>DNS Master / Slave Deployment</strong></li>
</ul>


<p>The communication servers infrastructure in Segundamano is done thought DNS, we do not pretend to explain what a DNS service is, let&rsquo;s just say that if you only use for communication IP addressing and for any reason you have to change the database IP, you will have to change the IP in every single server that has communication with that servers and your memory factor is an issue, because you can forget a server that you never touch but it is crucial and yeap! Your platform becomes unstable or with abnormal behavior.</p>

<p>One of our main directives in Segundamano is to have (at least in production) High Availability in the main services that provides support to our platform. We are not going to enter in detail about what BCP and DRP are (Business Continuity Plan and Disaster Recovery Plan respectively), but as you may know we need to be prepared in case of a server failure or Data Center crash, that&rsquo;s why naming resolution service is critical to have it up &amp; running <em>7x24x365</em>.</p>

<p>So when we create a virtual machine (via kickstart) this is enrolled automatically to FreeIPA server and at the same time its DNS name is created (Record A and PTR). FreeIPA acts as the master DNS server and we create separate DNS Bind instances or servers (as you want to call them) which acts as the slaves and they receive the configuration from its master (FreeIPA) automatically.</p>

<p>Hey wait a minute, you have just said before in this article that this is a centralized architecture about Authentication, NTP, DNS Tool Integration! The answer is yes, in Segundamano we centralize these services in FreeIPA to do it only once and not hundreds of times, but we also believe in Distributed Systems and Automation, so we centralize many of our main services in FreeIPA and this &ldquo;guy&rdquo; distribute its own configuration to others in order to have High Availability, that is what becomes super cool FreeIPA.</p>

<p>Let&rsquo;s see the next diagram to have a better idea about DNS Master/Slave Replication:</p>

<p><img src="http://backstage.segundamano.mx/images/dns_master_slave.jpg" alt="DNS master/slave" /></p>

<ul>
<li><strong>Network Time Protocol (NTP) Synchronization</strong></li>
</ul>


<p>Talking about Network Time Protocol, we are using the one that FreeIPA includes, so maybe you are wondering about why the NTP protocol is very important? And the answer is that NTP allows to synchronize the date of all your infrastructure in order to have more accurate logs and processes (and some systems depend on time).</p>

<p>If you want to have a reliable and synchronized infrastructure maybe you have to consider pointing all your infrastructure to a NTP Server. And the main advantage that FreeIPA&rsquo;s NTP offers is the possibility to audit every system you manage, having accurate information about them.</p>

<ul>
<li><strong>User Management</strong></li>
</ul>


<p>We have a centralized user database, so we create a user only once in FreeIPA and we are able to log in across all different systems in the company. This have multiple benefits for us as systems administrators, such as:</p>

<p><strong>Modularity:</strong> FreeIPA allow us to create user and system groups. This give us the flexibility to create Access Policies at user or system level, going from general to specific. Also give us the ability to set up policies accordingly to the real roles in the company.</p>

<p><strong>Reuse of Policies:</strong> once we create a policy it can be applied to multiple scenarios, avoiding the manual replication and possible human errors by doing that.</p>

<ul>
<li><strong>Free IPA Master / Replica Deployment</strong></li>
</ul>


<p>This probably one of the most super cool features about FreeIPA: the capability to create as many FreeIPA instances as we need. Why do I have to create another replica? Well there are tons of reasons, we think that the main reason is&hellip; yes you guest right! High Availability.</p>

<p>At this point you maybe wondering based on the point where we discuss earlier in this article about DNS, if I can create many replicas, why to create Bind DNS instances if with the replicas should be enough? For sure you can use replicas to support with DNS, NTP, Authentication, etc. in a HA schema in a distributed way, that is fine if you want to do it that way. For Segundamano we want to have a Centralized Manager which gathers and command all the services mentioned and at the same time have independent services which receives the information they need from its master, in this case FreeIPA, because if, in some case we need to restart or reload a service, we only need to to do it in the service involved and not restart or reload the entire stack of FreeIPA services!</p>

<p>Going back to FreeIPA replica, let?s say that the hard part was to create from zero the first FreeIPA server, create a replica is super simple and after some minutes automagically you will have a clone of your master and could be promoted itself as a master (you can only have one master at the time), all replicas can be promoted as master whenever you need it, ain&rsquo;t that cool!</p>

<p>These are the steps to create a replica:</p>

<ol>
<li><p>In the master just enter the next line:</p>

<p> <em>ipa-replica-prepare your_ipa_server_name</em></p>

<p>it creates a file named replica-info-your_ipa_server_name</p></li>
<li><p> In the slave:</p>

<p><em>ipa-replica-install replica-info-your_ipa_server_name &ndash;setup-ca &ndash;setup-dns &ndash;no-forwarders &ndash;skip-conncheck</em></p>

<p><strong>&ndash;setup-ca.-</strong> put this parameter in order to promote your replica as master when you need it. This parameter is optional, but take into account that when you want to promote your replica, you will need to install the CA information at that moment.</p>

<p><strong>&ndash;setup-dns.-</strong> this parameter is to clone your DNS configuration to your replica as well as replicate DNS information.</p></li>
</ol>


<p>And that?s it!</p>

<p>In the next diagram we show you a brief overview about schema replication in Segundamano:</p>

<p><img src="http://backstage.segundamano.mx/images/freeipa_replication.jpg" alt="FreeIPA replication" /></p>

<h2><strong>Integration with OKTA</strong></h2>

<p>OKTA is a tool that once is integrated with an LDAP Directory allows us to have a centralized access and management of a predefined set of commercial applications such as Gmail, Google Apps, Slack, Github, Sales Force, etc. In Segundamano we had integrated FreeIPA as our LDAP Server with OKTA. After this we can login in all internal and external systems handled by OKTA with the same login information (user and password).</p>

<p>With this implementation we are able to manage our user database more efficiently and we can create policies which will rule the permissions to access the applications and systems based on the profiles of every person in the company.</p>

<p><strong>Advantages</strong></p>

<ul>
<li><p>A single access with the same password (SSO Single Sign ON)</p></li>
<li><p>Automatic Updates App of Web Apps (OKTA is responsible for updating all)</p></li>
<li><p>Integration of new tools (OKTA is continually developing new applications compatibility)</p></li>
</ul>


<p><img src="http://backstage.segundamano.mx/images/okta_apps.png" alt="OKTA apps" /></p>

<p><strong>Implementation</strong></p>

<p>Our experience implementing LDAP OKTA made us realize we must know beforehand the main parameters of our LDAP deployment</p>

<ul>
<li>orgUrl</li>
<li>ldaphost</li>
<li>LDAPPort</li>
<li>ldapAdminDN</li>
<li>ldapAdminPassword</li>
<li>baseDN</li>
</ul>


<p>It is essential to have a basic knowledge about LDAP (FreeIPA) for reliable deployment. Also we need to install an agent in FreeIPA server so you can have communication with OKTA (cloud), once the configuration is done in OKTA you have to log in at portal as an administrator.</p>

<p>The OKTA configuration is managed from FreeIPA (OKTA uses and agent to check everything is setup correctly), this is a simple way to describe the implementation of OKTA with LDAP.</p>

<p>Once the OKTA agent is configured in our LDAP it was necessary to enable part of SSO in Google Apps to access all our applications in Google, this will instruct Google to use the OKTA credentials instead of their own.</p>

<p>With this technique we avoid having several accesses and passwords for different tools.</p>

<p><img src="http://backstage.segundamano.mx/images/okta_architecture.png" alt="OKTA architecture" /></p>

<h2><strong>Expected Results</strong></h2>

<p>With this solution we should be able to manage easily and without waste of time the everyday applications by having a central point of control and giving the users a portal where they can access the apps.</p>

<p>This needs applications supporting OKTA integration, at the moment we&rsquo;re handling through this system the following apps:</p>

<ul>
<li>Google Aps</li>
<li>Gmail</li>
<li>Google Drive</li>
<li>Google Calendar</li>
<li>Slack</li>
<li>Yammer</li>
<li>Box</li>
<li>Github</li>
<li>Jira</li>
<li>Confluence</li>
<li>Sales Force (In process)</li>
</ul>


<p><img src="http://backstage.segundamano.mx/images/okta_supported_apps.png" alt="OKTA supported apps" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hackamano, Segundamano.mx's Own Hackathon Is Coming!]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/05/27/hackamano/"/>
    <updated>2015-05-27T08:26:26-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/05/27/hackamano</id>
    <content type="html"><![CDATA[<p>Segundamano.mx is organazing its very own Hackathon: Hackamano 2015.
The event will take place at the ITAM University campus in Mexico City
the 20 &amp; 21 June 2015.</p>

<p>The idea is that anyone can participate and be part of a team, with
a very ambitious goal in mind: develop solution to today&rsquo;s problems in
local communities.</p>

<p>This can span from improving access to education, water supply, waste
management, public transportation or whatever you think it makes sense
in your social context.</p>

<p>This will be the only rule, everything else will be limited by your own
imagination!</p>

<p>To get more information about the event, please refer to the <a href="http://hackamano.segundamano.mx">official page</a>. The registration page is also up and running <a href="http://bit.ly/hackamano-registro">here</a>. Places are limited, go register now!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Follow Us on GitHub!]]></title>
    <link href="http://backstage.segundamano.mx/blog/2014/10/21/follow-us-on-github/"/>
    <updated>2014-10-21T10:39:11-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2014/10/21/follow-us-on-github</id>
    <content type="html"><![CDATA[<p>Segundamano.mx is now on GitHub. Follow our <a href="https://github.com/SegundamanoMX">Organization Page</a>.</p>

<p>The first repository that we have publised is this blog itself, whose source code
will be available at <a href="https://github.com/SegundamanoMX/octopress">this location</a>
and we hope that with time we&rsquo;ll be able to publish more bits and pieces, including contributions
to third party FLOSS, our own software, and some challenges that we&rsquo;ll tackle together with some
local communities.</p>

<p>Stay tuned, and feel free to fork and contribute back on any of our public repositories!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgresql Metrics With Logstash]]></title>
    <link href="http://backstage.segundamano.mx/blog/2014/07/07/postgresql-metrics-pipeline/"/>
    <updated>2014-07-07T08:00:59-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2014/07/07/postgresql-metrics-pipeline</id>
    <content type="html"><![CDATA[<p>I thought I&rsquo;d share our setup at <a href="http://www.segundamano.mx">SegundaMano.mx</a>
for extracting PostgreSQL metrics with <a href="http://www.logstash.net">Logstash</a> to
push to graphs. We&rsquo;re planning on upgrading our database cluster from an older
PostgreSQL version, and we want to know what potential performance bottlenecks
we have before and after upgrading</p>

<h2>Goals</h2>

<p>Our goals here are to answer the questions:</p>

<ul>
<li>Which of our queries are slow?</li>
<li>What&rsquo;s the ratio of slow queries to non-slow queries?</li>
<li>What&rsquo;s the average query time?</li>
</ul>


<p>We want these in place before we do a major overhaul of our databases,
so we can see how our upgrades perform. Graphs are always better than
&ldquo;well it feels a lot better&rdquo;</p>

<p>Our choice of tooling landed on <a href="http://logstash.net">Logstash</a>
parsing the CSV log, extracting data to push to Statsd/Graphite and to
Elasticsearch/Kibana via RabbitMQ. In addition to Logstash,
other tools can extract data from the CSV log, for example
<a href="http://dalibo.github.io/pgbadger">pgBadger</a> and PostgreSQL
itself.</p>

<p>From the CSV log, we extract the query duration of all queries and
push this to statsd. We also fire off a counter for each query, and a
separate for each slow query.</p>

<h2>Parts involved</h2>

<p>We broke this down into 4 different roles - this might change at some
later point but it&rsquo;s a start. I drew a little picture with the various
components - the dotted lines denote host boundaries</p>

<p><img src="http://backstage.segundamano.mx/images/logstash_pipeline.png"></p>

<h3>Database servers</h3>

<ul>
<li>Postgres with CSV logging</li>
<li>Logstash</li>
<li>Statsd</li>
</ul>


<h3>RabbitMQ servers</h3>

<ul>
<li>RabbitMQ in a cluster</li>
</ul>


<h3>Graphite server</h3>

<ul>
<li>Graphite (carbon-cache, whisper, graphite-web)</li>
</ul>


<h3>Elasticsearch server:</h3>

<ul>
<li>Logstash indexer</li>
<li>Elasticsearch</li>
<li>Kibana</li>
<li>Nginx</li>
</ul>


<h2>The pipeline components</h2>

<h2>Logstash</h2>

<p><a href="http://logstash.net">Logstash</a> is basically a Unix pipe on steroids.
It&rsquo;s a JRuby project with a lot of input, filter, and output plugins
hosted by Elasticsearch. It really shines when connected with
ElasticSearch and Kibana.</p>

<p>Usually what&rsquo;s done is a logstash &ldquo;agent&rdquo; instance runs on each server
that parses the logs, mangles them into your correct format, and push
them to a central broker. Then an &ldquo;indexer&rdquo; instance pulls them off
the broker and indexes them into ElasticSearch. This decouples the
instances a bit, allowing you to firewall off the ES instance from the
servers, or use a lightweight shipper agent lacking ES support (such
as <a href="http://beaver.readthedocs.org/en/latest/">Beaver</a>) to ship
entries.</p>

<p>As for brokers, the most common are Redis and RabbitMQ. The easiest to
use is Redis, using a queue or a channel. Unfortunately this doesn&rsquo;t
offer the routing possibilites of RabbitMQ, or the ability to see
events coming over in real time with a script performing <code>tail -f</code>
duties - with Redis you can see the firehose with <code>MONITOR</code> and that&rsquo;s it.</p>

<h2>RabbitMQ</h2>

<p><a href="http://www.rabbitmq.org">RabbitMQ</a> is an open source AMQP broker. We
have a separate Vhost for logstash, and separate users for indexer and
publisher.</p>

<p>In the vhost there&rsquo;s a persistent <code>topic</code> exchange. This means that we
can shut down either the producer or consumer without losing events on
the floor - they get queued up until the indexer comes alive again.
For availability we run a cluster over several hosts using a virtual
IP with keepalived for the clients to connect to.</p>

<h2>Statsd</h2>

<p><a href="https://github.com/etsy/statsd/">Statsd</a> is a project from Etsy. You
shove metrics to it (types available
<a href="https://github.com/etsy/statsd/blob/master/docs/metric_types.md">here</a>),
and it handles the per-timeunit bucketing, mean/std/upper90 of timers
et c and ships data to Graphite. Logstash has this as well in the
<a href="http://logstash.net/docs/1.4.1/filters/metrics">metrics</a> filter, but
it doesn&rsquo;t behave as well as I&rsquo;d like.</p>

<p>We use Vimeo&rsquo;s
<a href="https://github.com/vimeo/statsdaemon">go implementation</a> instead of
Etsy&rsquo;s NodeJS version. The main reasoning being go projects compile to
a static binary, simplifying deploys (since we don&rsquo;t use NodeJS in
production). I built a
<a href="https://gist.github.com/lflux/934eff42924e276c8673">RPM specfile</a> to
build an RPM of this.</p>

<h2>Graphite</h2>

<p><a href="http://graphite.readthedocs.org/">Graphite</a> is a graphing system
comprising several parts. Carbon-cache receives the metrics and stores
them in Whisper. Whisper is a timeseries database that doesn&rsquo;t lose resolution,
like RRD does. Graphite-web is a Django project that handles the user
interface and rendering.</p>

<p>We use PostgreSQL as the storage backend instead of
sqlite after reading <a href="http://obfuscurity.com/2013/12/Why-You-Shouldnt-use-SQLite-with-Graphite">this blog post</a></p>

<h2>Elasticsearch / kibana</h2>

<p>Logstash publishes the events into
<a href="http://www.elasticsearch.org">ElasticSearch</a>, with one index per
day. <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a> is a
HTML/JS frontend to Elasticsearch for viewing the log data. The nice
thing about Kibana is that it&rsquo;s really easy to search in the data and
randomly play around with different queries - it starts off with all
events and has nice breakdowns of the values of each. At my home
company, we use Kibana to make sense of the errors coming off our app
servers and have caught a lot of interesting bugs from randomly
playing around with the queries.</p>

<h2>Results</h2>

<p>A few hours after we enabled the whole pipeline, we already could use
the Kibana interface to spot slow queries, specifically two major
offenders that we could clear up with one index on a table.</p>

<p>Kibana slow query count:
<img src="http://backstage.segundamano.mx/images/kibana_slows.png"></p>

<p>Graphite queries vs slow queries
<img src="http://backstage.segundamano.mx/images/query_count.png"></p>

<p>Graphite query time
<img src="http://backstage.segundamano.mx/images/query_time.png"></p>

<!-- more -->


<h2>Configuration Details</h2>

<h2>On the PostgreSQL server</h2>

<h3>postgresql.conf</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Log both to syslog and CSV. A .log file will be created, but it will
</span><span class='line'>be empty.
</span><span class='line'>log_destination = 'syslog,csvlog'
</span><span class='line'>log_filename = 'postgresql-%Y-%m-%d.log'
</span><span class='line'># Log duration of all statements. 
</span><span class='line'># Log full statement of any that takes  more than 2 seconds.
</span><span class='line'>log_duration = on
</span><span class='line'>log_min_duration_statement = 2000
</span><span class='line'>log_statement = 'none'
</span><span class='line'># If you log in a locale such as es_MX, logstash might not parse the CSV correctly
</span><span class='line'>lc_messages = 'C'</span></code></pre></td></tr></table></div></figure>


<h3>Logstash shipper</h3>

<figure class='code'><figcaption><span>logstash-postgresql-shipper.conf</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>        file {
</span><span class='line'>                "path" =&gt; "/path/to/pg_log/*.csv"
</span><span class='line'>                "sincedb_path" =&gt; "/path/to/pg_log/sincedb_pgsql"
</span><span class='line'># fix up multiple lines in log output into one entry              
</span><span class='line'>           codec =&gt; multiline {
</span><span class='line'>                   pattern =&gt; "^%{TIMESTAMP_ISO8601}.*"
</span><span class='line'>                   what =&gt; previous
</span><span class='line'>                   negate =&gt; true
</span><span class='line'>           }
</span><span class='line'>        }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'># See http://www.postgresql.org/docs/9.3/interactive/runtime-config-logging.html#RUNTIME-CONFIG-LOGGING-CSVLOG
</span><span class='line'>        csv {
</span><span class='line'> columns =&gt; [  "log_time", "user_name", "database_name", "process_id", "connection_from", "session_id", "session_line_num", "command_tag", "session_start_time", "virtual_transaction_id", "transaction_id", "error_severity", "sql_state_code", "sql_message", "detail", "hint", "internal_query", "internal_query_pos", "context", "query", "query_pos", "location"]
</span><span class='line'>        }
</span><span class='line'>  mutate {
</span><span class='line'>         gsub =&gt; [ "sql_message", "[\n\t]+", " "]
</span><span class='line'>  }
</span><span class='line'># use timestamp from log file  
</span><span class='line'>  date {
</span><span class='line'>        #2014-05-22 17:02:35.069 CDT
</span><span class='line'>        match =&gt; ["log_time", "YYYY-MM-dd HH:mm:ss.SSS z"]
</span><span class='line'>}
</span><span class='line'>  grok {
</span><span class='line'>    match =&gt; ["sql_message", "duration: %{DATA:duration:int} ms"]
</span><span class='line'>    tag_on_failure =&gt; []
</span><span class='line'>        add_tag =&gt; "sql_message"
</span><span class='line'>  }
</span><span class='line'># See postgres configuration - a message with 'statement: ' is a slow query
</span><span class='line'>  grok  {
</span><span class='line'>        match =&gt;["sql_message", "statement: %{GREEDYDATA:statement}"]
</span><span class='line'>        tag_on_failure =&gt; []
</span><span class='line'>        add_tag =&gt; "slow_statement"
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'># Increase hitcounter when we see a slow statement
</span><span class='line'>  if "slow_statement" in [tags] {
</span><span class='line'>        statsd {
</span><span class='line'>                increment =&gt; "postgresql.slow_queries"
</span><span class='line'>        }
</span><span class='line'>  }
</span><span class='line'>  # increase hitcounter for all queries
</span><span class='line'>  # send timing metrics for queries
</span><span class='line'>  if "sql_message" in [tags] {
</span><span class='line'>          statsd {
</span><span class='line'>                timing =&gt; {"query_duration" =&gt; "%{duration}"}
</span><span class='line'>                increment =&gt; "postgresql.queries"
</span><span class='line'>          }
</span><span class='line'>  }
</span><span class='line'>  # Ship off all to rabbitmq
</span><span class='line'>  rabbitmq {
</span><span class='line'>    'durable' =&gt; true
</span><span class='line'>    'exchange' =&gt; 'logstash'
</span><span class='line'>    'exchange_type' =&gt; 'topic'
</span><span class='line'>    'host' =&gt;  "server"
</span><span class='line'>  #note - replace type and host manually since as of writing this isn't replaced like it should be
</span><span class='line'>    'key' =&gt; 'logstash.%{type}.%{host}'
</span><span class='line'>    'password' =&gt; "password"
</span><span class='line'>    'persistent' =&gt; true
</span><span class='line'>    'user' =&gt; 'shipper'
</span><span class='line'>    'vhost' =&gt; 'logstash'
</span><span class='line'>    'workers' =&gt; 4
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>Statsdaemon</h3>

<p>Not much to this here - just run statsdaemon and edit the configuration file <code>/etc/statsdaemon.ini</code> and set the correct graphite host.</p>

<h2>RabbitMQ</h2>

<p>Set up rabbitmq with a Vhost for logstash, create users indexer and
shipper that can read and write to the logstash vhost. Before starting
the shipper, start the indexer which will automatically create the
binding from the exchange to the queue.</p>

<h2>ElasticSearch/Indexers</h2>

<h3>ElasticSearch</h3>

<p>Install elasticsearch with a fair bit of storage.</p>

<figure class='code'><figcaption><span>/etc/elasticsearch/elasticsearch.yml</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">cluster.name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">elasticsearch</span>
</span><span class='line'><span class="l-Scalar-Plain">path.conf</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/etc/elasticsearch</span>
</span><span class='line'><span class="l-Scalar-Plain">path.data</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/opt/logs/elasticsearch/data</span>
</span><span class='line'><span class="l-Scalar-Plain">path.logs</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/opt/logs/elasticsearch/logs</span>
</span><span class='line'><span class="l-Scalar-Plain">discovery.zen.minimum_master_nodes</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">1</span>
</span><span class='line'><span class="l-Scalar-Plain">discovery.zen.ping.multicast.enabled</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Logstash Indexer</h3>

<p>Configure to pull events off RabbitMQ and index them into Elasticsearch</p>

<figure class='code'><figcaption><span>/opt/logstash/server/etc/conf.d/logstash.conf</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>
</span><span class='line'>      rabbitmq {
</span><span class='line'>        'auto_delete' =&gt; 'false'
</span><span class='line'>        'codec' =&gt; 'json'
</span><span class='line'>        'debug' =&gt; true
</span><span class='line'>        'durable' =&gt; true
</span><span class='line'>        'exchange' =&gt; 'logstash'
</span><span class='line'>        'exclusive' =&gt; false
</span><span class='line'>        'host' =&gt; 'host'
</span><span class='line'>        'key' =&gt; 'logstash.#'
</span><span class='line'>        'password' =&gt; 'passwd'
</span><span class='line'>        'queue' =&gt; 'logstash-indexer'
</span><span class='line'>        'user' =&gt; 'indexer'
</span><span class='line'>        'vhost' =&gt; 'logstash'
</span><span class='line'>      }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch { host =&gt; "host"
</span><span class='line'>        cluster =&gt; "logstash"
</span><span class='line'>        protocol =&gt; "http"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>Kibana</h3>

<p>Install kibana from <a href="https://github.com/elasticsearch/kibana">git master</a> into the directory
where you want it served from.</p>

<h3>Nginx</h3>

<p>Set up Nginx to show kibana HTML and to proxy requests to Elasticsearch</p>

<figure class='code'><figcaption><span>/etc/nginx/sites-enabled/kibana</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class='nginx'><span class='line'><span class="k">server</span> <span class="p">{</span>
</span><span class='line'>  <span class="kn">listen</span>                <span class="n">host</span><span class="p">:</span><span class="mi">80</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="kn">server_name</span>           <span class="s">host</span><span class="p">;</span>
</span><span class='line'>  <span class="kn">access_log</span>            <span class="s">/var/log/nginx/kibana.log</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="kn">location</span> <span class="s">/</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">root</span>  <span class="s">/opt/kibana/current/src</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">index</span>  <span class="s">index.html</span>  <span class="s">index.htm</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_aliases$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_aliases$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_nodes$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_search$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_mapping$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_cluster/health$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># password protected end points</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/kibana-int/dashboard/.*$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/kibana-int/temp.*$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
</feed>
