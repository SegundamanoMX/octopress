<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Segundamano's Backstage]]></title>
  <link href="http://backstage.segundamano.mx/atom.xml" rel="self"/>
  <link href="http://backstage.segundamano.mx/"/>
  <updated>2015-07-01T23:09:13-05:00</updated>
  <id>http://backstage.segundamano.mx/</id>
  <author>
    <name><![CDATA[Segundamano.mx Tech Team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Continuous Delivery]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/07/01/continuous-delivery/"/>
    <updated>2015-07-01T16:08:49-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/07/01/continuous-delivery</id>
    <content type="html"><![CDATA[<p>Continuous Delivery is a software development approach in which teams keep producing code in short cycles and ensure that the software can be reliably and released at any time.</p>

<p>What&rsquo;s Blue/Green deploy strategy? There&rsquo;s a lot of different opinions and definitions, for us we see it as a way to avoid being offline if something goes wrong with a new deploy of functionality (and being shouted at, or fired).</p>

<p>The basic premise is to duplicate your production stack in order to have one with the most recent software installed and the other with the last stable version (and proved in production). This way if something happens with the new deploy you can always perform a rollback as fast as you can switch traffic to the old stack.</p>

<p>About the &ldquo;blue&rdquo; and &ldquo;green&rdquo; concepts, sometimes the new version is called blue while green is the fallback; for some people is the other way around. That&rsquo;s why we&rsquo;ll use <strong>fallback</strong> to describe the &ldquo;old, stable&rdquo; version of the system while we&rsquo;ll use <strong>canary</strong> to describe a new, shining and hopefully better version of it.</p>

<p>This brings another concept: canary.</p>

<p>It&rsquo;s simply to use a chunk of stack for deploying new versions. This way you&rsquo;ll have a canary in your stack in which validation over production will be performed. It&rsquo;s refered as canary for the real-live canaries used in mines to &ldquo;supervise&rdquo; the quality of the air, if the canary dies (because the air is rare) the miners could have time to be saved.</p>

<p>The canary part of the stack gradually becomes bigger and bigger and at some point your whole stack is canary. This way you tested progressively in production a new feature without the risk of putting your site offline.</p>

<h2><strong>Current Situation</strong></h2>

<p>Currently we are working with a development methodology in which new features are delivered to production twice a month and usually with a lot of features. This practice increased the complexity to resolve a problem: as many features you develop is as many components you have to troubleshoot in case of a problem and you don&rsquo;t know which one caused the problem.</p>

<p>When a new feature is developed it has to wait until the next release (two weeks) to be in production, this makes our delivery process very slow.</p>

<p>In the past we had to install all the new developed packages manually at our infrastructure, this means enter in every single server involved for the release process, now we are using a super cool tool which is called SaltStack as a configuration and orchestration management system, this tool allows us to install automatically and quickly new version of packages.</p>

<p>If you are familiar with Puppet or Chef you would know a little bit more about what we are talking about, if not, you can search by your own the configuration and remote execution management tool that suites your needs.</p>

<p>The Segundamano infrastructure seeing from the point of view of the interaction between Front Servers (www, php, nginx, varnish, etc) and the End-User&rsquo;s request, begins with the firewalls as a first level that includes security for layer 3 and 4 and which redirect (via DNAT) to a second level composed for Load Balancers with High Availability (HA) capabilities, in other words these load balancers in our case Keepalived can create a virtual IP Address (VRRP) and they have the task besides the HA to balance at layer 3 and 4 the user&rsquo;s request for the next level composed by the Front platform, currently there are two more lower levels which are the backend, and finally the Data Layer (Database.- PostgreSQL),  but they are not touched by the load balancing layer  as we can see in the next Diagram:</p>

<p><img src="http://backstage.segundamano.mx/images/as_is_infrastructure.jpg" alt="Infrastructure as is" /></p>

<h2><strong>Goals</strong></h2>

<p>Now, there are many ways to conceptualize the Blue/Green/Canary paradigm.  Depending on your needs, current situation and obviously  your infrastructure architecture.</p>

<p>Based on the idea presented in the introduction of this article and having in mind our current situation and the infrastructure that Segundamano has, we decided to represent and therefore to implement Blue/Green/Canary <em>under the concept that all frontals servers can be a potential candidate to have the role or belong to one of the Stacks mention before, this is to be a  Blue, Green or Canary server</em>.</p>

<p>Yea you are right !!, this sounds so simple, but there are some key points to have in mind in order to pass through correctly from the concept to the implementation:</p>

<ol>
<li><p>When we want to release a new potential version,  we just want to redirect a small percentage of the user&rsquo;s request in order to test functionality (<strong>Canary Stack</strong>).</p></li>
<li><p>Then if everything is going well, we are going to redirect the 100% of the user&rsquo;s request to the new potential release version of our platform.(<strong>Blue Stack</strong>)</p></li>
<li><p>But not always life is good, what if for any reason, and there are tons of them, we realized that some feature or functionality it&rsquo;s broken after we have been redirected users request to the new potential release version !!, Well, we need to redirect them back to the Stable Version we use to have (<strong>Fallback Stack</strong>)</p></li>
<li><p>We need to ensure that in the process of redirecting users request from one stack to another, the user&rsquo;s sessions are not going to be lost or closed.</p></li>
</ol>


<h2><strong>Implementation </strong></h2>

<ol>
<li><p>As we mentioned before, we must create one more stack of servers, for the new potential releases, in this case will be the Blue Stack, we already have the Green Stack  for the current release stable version, which in fact  is the only one we have today.</p></li>
<li><p>Hey wait a minute, but you have just  said  that all servers can be a candidate to be or belong for any Stack no matter if we are talking about Blue, Green or Canary, Yes you are right !!, but we need resources in order to support at least two different versions alive at same time of the platform (web, msite, etc).</p></li>
<li><p>We also need to create an extra layer of load balancers, <em>but Why another one!? if we already have the super powerful Keepalived load balancers</em>. Well, as we described earlier in this article, those  load balancers ensures HA and 3 and 4 layer load balancing (IP/TCP), but &hellip; and this is a Big But !!, we are not ensuring the users sessions layer, for tech guys this is the OSI layer 7, this means that we need to ensure that the user&rsquo;s sessions will not be lost or closed if we move them from one Stack to another.</p></li>
<li><p>So, based on the point above, we selected after some studies on the performance and usability benchmarking as well as our own know how in some tools a proxy tool which has  two main advantages:</p>

<ol>
<li><p>Layer 7 support load balancing, this is the persistence based on cookies (sessions).</p></li>
<li><p>A proxy server which can be programmable (dynamic behavior).</p></li>
</ol>
</li>
</ol>


<p>We describe in the next diagram the idea explained in the points discussed before:</p>

<p><img src="http://backstage.segundamano.mx/images/infrastructure_to_be.jpg" alt="Infrastructure to be" /></p>

<p>And talking about Load Balancing levels and layers, why not just remove the keepalived level which are the load balancers for layer 3 and 4 and just leave the new Layer 7 proxy load balancers. Well, if we want a full redundant architecture, let&rsquo;s remember that for example HAProxy or Nginx or Pen or Pound they do not support the creation of a Virtual IP Address which ensures the HA feature, and if in Segundamano already have the keepalived which has this feature, the question would be, Why not combine the features from one Load Balancer (HA and 3 and 4 layer support) and the other Load Balancer (Programmable and layer 7 cookies session support)?.</p>

<p>And &hellip; that&rsquo;s it !!, Is that all?, well, the answer is NO, let&rsquo;s remember that almost all things in life or at least in projects are about PPT.- People, Process and Tools.</p>

<p>We need a tool to help us to distribute to a big amount of servers the proper configuration that enable us to change the behavior on the fly (orchestration), for example to move a certain % of users from one stack to another and vice versa.</p>

<p>But all tools and People involved are useless if we do not define the correct processes that must be reflected on the tools we use.</p>

<p>In the next section we will talk a little bit more about the technologies we decided to use in order to achieve the Blue/Green/Canary paradigm.</p>

<p>One note, we always got confused when we tried to identify what&rsquo;s blue or what&rsquo;s green? that&rsquo;s why we stopped thinking like that and use the canary and fallback concepts; after this blue and green are only names of the stack, but as that is really, really boring we came to name it &ldquo;Topo&rdquo; and &ldquo;Gigio&rdquo; after an inside joke in the team as one of our colleagues is a lookalike of the TV program.</p>

<p>So, <strong>topo</strong> or <strong>gigio</strong> can be at anytime <strong>canary</strong> or <strong>fallback</strong> stack per service basis (but not the two at the same time).</p>

<p> <img src="http://backstage.segundamano.mx/images/topogigio.jpg" alt="Topo Gigio" /></p>

<h2><strong>Tools</strong></h2>

<p>In order to implement this solution, we had to choose the right tools that help us to achieve our goals. So we choose the following tools:</p>

<ul>
<li><strong>Salt Stack </strong></li>
</ul>


<p>Salt Stack is a python based open source configuration management and remote execution application which handle the infrastructure as code.</p>

<p>We used this tool as the server&rsquo;s brain, so we can handle our entire infrastructure from a single point, allowing us to standardize our servers configuration and to automate tasks.</p>

<p>For automation we used salt stack with jinja, that is a python templating extension that enable us to make dynamic and on the fly configuration files for the servers.</p>

<ul>
<li><strong>Keepalived</strong></li>
</ul>


<p>Keepalived is an open source Layer 3 and 4 Load Balancer Application for incoming requests between servers. The main goal of this component is to provide high availability to Linux system based Infrastructures.</p>

<p>We use Keepalived in order to provide high availability to our Proxy Load Balancers (Nginx), because of Nginx can&rsquo;t do it by itself.</p>

<ul>
<li><strong>Nginx</strong></li>
</ul>


<p>Nginx is a Free, Open Source and high performance Http server and proxy.In our experience, it has been considered the best option for handling large amount of requests.</p>

<p>We use Nginx as load balancer and as SSL Proxy. Basically all the requests that come from our end users are handled by Nginx in order to secure them via the SSL protocol.</p>

<ul>
<li><strong>Python</strong></li>
</ul>


<p>Python is a high-level programming language that has been created to emphasize the code readability and its syntax allows programmers to express concepts in fewer lines of code.</p>

<p>We use Python to create the script which orchestrate dynamically the load balancer&rsquo;s configuration (with help of SaltStack-Jinja) in order to follow with the continuous delivery process.</p>

<h2><strong>Configuration Details</strong></h2>

<p><strong>HA Proxy</strong>:</p>

<p>We start our project with HA Proxy in mind. It&rsquo;s simple, reliable and fast. Three key points we needed for balancing our traffic between stacks in order to have our &ldquo;virtual canary&rdquo;.</p>

<p>The configuration is really simple, with something like</p>

<pre><code>frontend  main *:443
    acl canary      cook(site_id) canary
    acl fallback    cook(site_id) fallback

    use_backend     canary      if canary
    use_backend     fallback    if fallback
    default_backend newuser

backend newuser
    balance roundrobin
    cookie  site_id     insert indirect maxlife 72h preserve
    server  fallback1   10.39.0.92:443 check inter 1000 cookie fallback weight 95
    server  fallback2   10.39.0.93:443 check inter 1000 cookie fallback weight 95
    server  canary1     10.39.1.90:443 check inter 1000 cookie canary weight 5
    server  canary2     10.39.1.90:443 check inter 1000 cookie canary weight 5

backend canary
    balance roundrobin
    cookie  site_id     insert indirect maxlife 72h preserve
    server  canary1     10.39.1.90:443 check inter 1000 cookie canary weight 1
    server  canary2     10.39.1.91:443 check inter 1000 cookie canary weight 1

backend fallback
    balance roundrobin
    cookie  site_id     insert indirect maxlife 72h preserve
    server  fallback1   10.39.0.92:443 check inter 1000 cookie canary weight 1
    server  fallback2   10.39.0.93:443 check inter 1000 cookie canary weight 1
</code></pre>

<p>What this does is to check if the user have a cookie site_id with values canary or fallback, if so it use the proper backend for each value of the cookie, if not we redirect it to the backend newuser and then we set the different weights of the stack directly in each server.</p>

<p>Anyway, we&rsquo;ll not get deeper into HA Proxy, as we soon found it only support session cookies. It can modify a cookie already set elsewhere, but if that cookie is missing (for example, a fresh user which have never visited our site) it will depend on the application to handle most of the logic.</p>

<p>We prefered to move to Nginx, as it can handle cookies with expiration date. It&rsquo;s not &ldquo;as simple&rdquo; as HA Proxy, but it give us a little of room to &ldquo;script&rdquo; the cookie part.</p>

<p><strong>Nginx</strong>:</p>

<ul>
<li><strong>Load balancing part:</strong></li>
</ul>


<p>We&rsquo;re using the load balancing features of Nginx, this is as &ldquo;simple&rdquo; as:</p>

<pre><code>upstream fallback {
    server          10.39.1.90 max_fails=5 fail_timeout=20;
    server          10.39.1.91 max_fails=5 fail_timeout=20;
}

upstream canary {
    server          10.39.0.93 max_fails=5 fail_timeout=20;
    server          10.39.0.92 max_fails=5 fail_timeout=20;
}
</code></pre>

<p>That give us the basic round-robin balancing of the servers in each upstream with the same weight. We set up the two stacks and then we can send traffic to them with</p>

<pre><code>location / {
    proxy_pass http://fallback$request_uri;
}
</code></pre>

<p>That force us to change configuration once we want to switch from stack (the blue/green paradigm we discuss before) and it doesn&rsquo;t allow to apply to it the concept of canary, since our way of seeing it is to make it work through balancing a portion of traffic to the new stack.</p>

<p>But in order to really replicate the features of HA Proxy we need to check if the cookie to handle the balancing between stacks is present, if not assign the new user to a stack and then proxy the connection to the right stack.</p>

<ul>
<li><strong>Cookie part:</strong></li>
</ul>


<p>For the cookie part we can use a map. As Nginx create a variable for each cookie received we only need to map it to a result. We came with the following code</p>

<pre><code>map $cookie_site_id $selected_upstream {
    default newuser;
    canary canary;
    fallback fallback;
}
</code></pre>

<p>That way, if the cookie does not exist or the value of the cookie is not &ldquo;canary&rdquo; or &ldquo;fallback&rdquo; it will return the default value of &ldquo;newuser&rdquo; in the &ldquo;selected_upstream&rdquo; variable, otherwise it will return &lsquo;canary&rsquo; or &lsquo;fallback&rsquo;.</p>

<p>After doing this we need to send the cookie to the user, for when he or she returns, it will use the same stack. For doing this the code is</p>

<pre><code>add_header Set-Cookie "site_id=$selected_upstream;Domain=.test.me;Path=/;Max-Age=99000";
</code></pre>

<p>Actually, after doing this, we have solved how to select &ldquo;on the fly&rdquo; the stack the user will use, now we can change the line to proxy the connection</p>

<pre><code>location / {
    proxy_pass http://$selected_upstream$request_uri;
}
</code></pre>

<p>We still need to have a way to balance the traffic between stacks. For that reason the default value of the map described above returns &ldquo;newuser&rdquo; when the cookie is not set.</p>

<ul>
<li><strong>Split Clients:</strong></li>
</ul>


<p>There are different ways of &ldquo;randomize&rdquo; the selected stack when a new user enters the site. We use a simple module of Nginx named <a href="http://nginx.org/en/docs/http/ngx_http_split_clients_module.html">split clients</a>, what it does basically is to hash an input string to a defined range of numbers and then check if the result of the hash is between a subrange in the predefined &ldquo;universe&rdquo; of the possible results. We use it because it is simple, but also using the perl module in Nginx to do calculation was considered; we don&rsquo;t need &ldquo;that much power&rdquo;.</p>

<p>As this gives always the same output and we wanted to get some more randomness, we send as the input string the local time (when the request was made), the IP and the User Agent. Only using IPs for some could solve the whole problem, but in our site a lot of users share the same IP and sometimes we want to deliver between them different versions of the site (hence, different stacks).</p>

<pre><code>split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version {
    1.0%    canary;
    *       fallback;
}
</code></pre>

<p>And we only need to use the output of the module (in the variable &ldquo;stack_version&rdquo;) to assign a first time user.</p>

<pre><code>if ($selected_upstream = "newuser") {
    set $selected_upstream $stack_version;
}
</code></pre>

<p>At this point we have an almost functional configuration, and the new users can be balanced between stacks by modifying the nginx configuration. For the logic we set up so far, moving users between stacks when they already have the cookie we need a flag to check what version of configuration they have.</p>

<pre><code>set $rollout_stage 1;
add_header Set-Cookie "site_rs=$rollout_stage;Domain=.test.me;Path=/;Max-Age=99000";
</code></pre>

<p>For moving users from the fallback stack (as we don&rsquo;t want to move the users from the canary, they already are in the new version) we can check if they belong to that stack and then again select a new one (probably).</p>

<pre><code>split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version_renew {
    1.0%    canary;
    *       fallback;
}

...

if ($cookie_site_rs != $rollout_stage) {
    set $candidate_split_clients "D";
}
if ($selected_upstream = "fallback"){
    set $candidate_split_clients "${candidate_split_clients}C";
}
if ($candidate_split_clients = "DC"){
    set $selected_upstream $stack_version_renew;
}
</code></pre>

<p>There are two peculiarities here, as Nginx is not capable of having &ldquo;OR&rdquo; or &ldquo;AND&rdquo; logic in the if statements, we need to check them separately (if the user is in &ldquo;fallback&rdquo; and if the user is in another rollout stage) and then check if both conditions are true.</p>

<p>The other is the need to use a separately split_modules to calculate the percentage of users which will move from &ldquo;fallback&rdquo; to &ldquo;canary&rdquo;. The percentage should be different (with the exceptions of 0% and 100%) for new users and existing ones.</p>

<p>Let&rsquo;s say we in the first stage want to deliver canary to 5% of the users. At this point all the users are new, so everybody will fall into the split for new users. Then, in the second stage of rollout we want to increase the percentage to 10%. For new users, setting 10% in the split is all that we need, but 95% of our existing users should be in the &ldquo;fallback&rdquo; stack. We only need to move 5.2631% of those users to canary in stage 2.</p>

<p>Talking with numbers (and some assumptions to understand this: 100 new users enter in each stage and all the previous users enter again); if in the first stage we have 100 users, 5 will go to &ldquo;canary&rdquo;, 95 will go to &ldquo;fallback&rdquo;. In the second stage, of the new users 10 will go to &ldquo;canary&rdquo; and 90 will go to &ldquo;fallback&rdquo;; for the old users of the first stage we only need to move 5 out of 95 from &ldquo;fallback&rdquo; to &ldquo;canary&rdquo;, that&rsquo;s or 5.2631% of fallback users.</p>

<table>
<thead>
<tr>
<th> Stage/Percentage  </th>
<th> New users &ldquo;C&rdquo;     </th>
<th> New users &ldquo;F&rdquo;     </th>
<th> Old users &ldquo;C&rdquo;         </th>
<th> Old users &ldquo;F&rdquo;     </th>
</tr>
</thead>
<tbody>
<tr>
<td> 1 - 5%            </td>
<td> 5                 </td>
<td> 95                </td>
<td>                       </td>
<td>                   </td>
</tr>
<tr>
<td> 2 - 10%           </td>
<td> 10                </td>
<td> 90                </td>
<td> 5 + 5 (5.2631%)       </td>
<td> 95 - 5            </td>
</tr>
<tr>
<td> 3 - 15%           </td>
<td> 15                </td>
<td> 85                </td>
<td> 20 + 10 (5.5555%)     </td>
<td> 180 - 10          </td>
</tr>
<tr>
<td> 4 - 20%           </td>
<td> 20                </td>
<td> 80                </td>
<td> 45 + 15 (5.8823%)     </td>
<td> 255 - 15          </td>
</tr>
<tr>
<td> 5 - 25%           </td>
<td> 25                </td>
<td> 75                </td>
<td> 80 + 20 (6.25%)       </td>
<td>                   </td>
</tr>
</tbody>
</table>


<p>The table shows us the following of the previous example. When we are in the third stage, we have 180 users in fallback and 20 in canary. We should have 170 in fallback and 30 in canary, so we need to move 10 users from fallback to canary, that&rsquo;s the 5.5555% of the 180 users.</p>

<p>In the fourth stage we have 255 users in fallback and 45 in canary, we should have 240 users in fallback and 60 in canary, so we need to move 15 users from fallback to canary, that&rsquo;s 5.8823% of the 255 users.</p>

<p>Finally, we have a configuration ready, which is:</p>

<pre><code>map $cookie_site_id $selected_upstream {
    default newuser;
    canary canary;
    fallback fallback;
}

split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version {
    1.0%    canary;
    *       fallback;
}

split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version_renew {
    1.0%    canary;
    *       fallback;
}

server {
    server_name                     m.test.me;
    listen 443 ssl;
    set $rollout_stage 1;
    if ($cookie_site_rs != $rollout_stage) {
        set $candidate_split_clients "D";
    }
    if ($selected_upstream = "fallback"){
        set $candidate_split_clients "${candidate_split_clients}C";
    }
    if ($candidate_split_clients = "DC"){
        set $selected_upstream $stack_version_renew;
    }
    if ($selected_upstream = "newuser") {
        set $selected_upstream $stack_version;
    }
    add_header Set-Cookie "site_id=$selected_upstream;Domain=.test.me;Path=/;Max-Age=99000";
    add_header Set-Cookie "site_rs=$rollout_stage;Domain=.test.me;Path=/;Max-Age=99000";

    location / {
        proxy_pass http://$selected_upstream$request_uri;
    }
}

upstream fallback {
    server          10.39.1.90 max_fails=5 fail_timeout=20;
    server          10.39.1.91 max_fails=5 fail_timeout=20;
}

upstream canary {
    server          10.39.0.93 max_fails=5 fail_timeout=20;
    server          10.39.0.92 max_fails=5 fail_timeout=20;
}
</code></pre>

<h2><strong>Expected Results</strong></h2>

<p>With the Continuous Delivery Implementation, we want to improve some important points in the company:</p>

<p><strong>Speed:</strong> We can deliver faster important improvements, fixes and features to our end users at any time, because we will be able to release some changes in hours or days. When we achieve this velocity our product may have fewer bugs and user experience will increase.</p>

<p><strong>Quality:</strong> This approach helps us learning faster about our errors so we can improve the thing we do and release products with more quality.</p>

<p><strong>Focus:</strong> All the team members can focus on finish and release a task before moving to another task. They don&rsquo;t need to switch their brains between tasks.</p>

<p><strong>Clarity:</strong> The team should experience less stress, because we have to fix small problems (if any) quickly so this conducts to a lower pressure work.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FreeIPA for Your Infrastructure Daily Operations]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/06/29/freeipa-control-access-and-okta/"/>
    <updated>2015-06-29T15:42:23-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/06/29/freeipa-control-access-and-okta</id>
    <content type="html"><![CDATA[<p>When you manage lots of servers (Unix and Linux like), dozens or hundreds of them, one thing for sure that you must take in account is the Access Control, and there are many many tools to help us to that specific task, but let&rsquo;s be realists!  In a minimal decent infrastructure implementation you don?t need to have only Access Control, you need as well policies depending on the user&rsquo;s profile. For example, you are not going to provide root permissions to a sales guy in a linux server for making queries to the main database or if you just want certain developers can execute a critical script (bash or python or whatever).</p>

<p>And we can say, come on! I have a tool that allows me to define Access Control as well as robust policies rules (<em>all mention before know as Identity and Policies</em>), for example LDAP or the well known MS Active Directory. But the Access Control is the only thing we have to take care about in our daily minimal server&rsquo;s operation?, obviously No! What about synchronization? this is to have all our server in the correct time for having accurate logs or naming resolution, if we want a friendly way to identify our servers, you know, things that are essentials in order to ensure that the basic communication and security its guaranteed.</p>

<p>Let&rsquo;s stop to talk about needs that aren?t new for us and let&rsquo;s talk about the solutions or in this case Segundamano&rsquo;s solution.</p>

<h2><strong>FreeIPA</strong></h2>

<p>We are not going to provide or copy/paste an explanation about what FreeIPA is from Wikipedia o Freeipa.org, in that case, please feel free to do it by yourself. We prefer to tell you in our experience what FreeIPA is doing for us. In Segundamano we use FreeIPA for the following approaches:</p>

<ul>
<li><strong>DNS Resolution</strong></li>
</ul>


<p>FreeIPA offers an integration of many open source components like Bind DNS Server, so we decided to use it as our main DNS Platform, In this way we can have multiple Bind slaves servers from it.</p>

<p>And why is this useful? Well It is useful because we want to have an instance for FreeIPA in each environment or Data Center in order to accelerate the DNS resolution and offer high availability.</p>

<p>Maybe you are wondering about what is a FreeIPA instance? Ok this topic will be explained on a next section on this document when we talk about replication which is a super nice feature.</p>

<ul>
<li><strong>DNS Master / Slave Deployment</strong></li>
</ul>


<p>The communication servers infrastructure in Segundamano is done thought DNS, we do not pretend to explain what a DNS service is, let&rsquo;s just say that if you only use for communication IP addressing and for any reason you have to change the database IP, you will have to change the IP in every single server that has communication with that servers and your memory factor is an issue, because you can forget a server that you never touch but it is crucial and yeap! Your platform becomes unstable or with abnormal behavior.</p>

<p>One of our main directives in Segundamano is to have (at least in production) High Availability in the main services that provides support to our platform. We are not going to enter in detail about what BCP and DRP are (Business Continuity Plan and Disaster Recovery Plan respectively), but as you may know we need to be prepared in case of a server failure or Data Center crash, that&rsquo;s why naming resolution service is critical to have it up &amp; running <em>7x24x365</em>.</p>

<p>So when we create a virtual machine (via kickstart) this is enrolled automatically to FreeIPA server and at the same time its DNS name is created (Record A and PTR). FreeIPA acts as the master DNS server and we create separate DNS Bind instances or servers (as you want to call them) which acts as the slaves and they receive the configuration from its master (FreeIPA) automatically.</p>

<p>Hey wait a minute, you have just said before in this article that this is a centralized architecture about Authentication, NTP, DNS Tool Integration! The answer is yes, in Segundamano we centralize these services in FreeIPA to do it only once and not hundreds of times, but we also believe in Distributed Systems and Automation, so we centralize many of our main services in FreeIPA and this &ldquo;guy&rdquo; distribute its own configuration to others in order to have High Availability, that is what becomes super cool FreeIPA.</p>

<p>Let&rsquo;s see the next diagram to have a better idea about DNS Master/Slave Replication:</p>

<p><img src="http://backstage.segundamano.mx/images/dns_master_slave.jpg" alt="DNS master/slave" /></p>

<ul>
<li><strong>Network Time Protocol (NTP) Synchronization</strong></li>
</ul>


<p>Talking about Network Time Protocol, we are using the one that FreeIPA includes, so maybe you are wondering about why the NTP protocol is very important? And the answer is that NTP allows to synchronize the date of all your infrastructure in order to have more accurate logs and processes (and some systems depend on time).</p>

<p>If you want to have a reliable and synchronized infrastructure maybe you have to consider pointing all your infrastructure to a NTP Server. And the main advantage that FreeIPA&rsquo;s NTP offers is the possibility to audit every system you manage, having accurate information about them.</p>

<ul>
<li><strong>User Management</strong></li>
</ul>


<p>We have a centralized user database, so we create a user only once in FreeIPA and we are able to log in across all different systems in the company. This have multiple benefits for us as systems administrators, such as:</p>

<p><strong>Modularity:</strong> FreeIPA allow us to create user and system groups. This give us the flexibility to create Access Policies at user or system level, going from general to specific. Also give us the ability to set up policies accordingly to the real roles in the company.</p>

<p><strong>Reuse of Policies:</strong> once we create a policy it can be applied to multiple scenarios, avoiding the manual replication and possible human errors by doing that.</p>

<ul>
<li><strong>Free IPA Master / Replica Deployment</strong></li>
</ul>


<p>This probably one of the most super cool features about FreeIPA: the capability to create as many FreeIPA instances as we need. Why do I have to create another replica? Well there are tons of reasons, we think that the main reason is&hellip; yes you guest right! High Availability.</p>

<p>At this point you maybe wondering based on the point where we discuss earlier in this article about DNS, if I can create many replicas, why to create Bind DNS instances if with the replicas should be enough? For sure you can use replicas to support with DNS, NTP, Authentication, etc. in a HA schema in a distributed way, that is fine if you want to do it that way. For Segundamano we want to have a Centralized Manager which gathers and command all the services mentioned and at the same time have independent services which receives the information they need from its master, in this case FreeIPA, because if, in some case we need to restart or reload a service, we only need to to do it in the service involved and not restart or reload the entire stack of FreeIPA services!</p>

<p>Going back to FreeIPA replica, let?s say that the hard part was to create from zero the first FreeIPA server, create a replica is super simple and after some minutes automagically you will have a clone of your master and could be promoted itself as a master (you can only have one master at the time), all replicas can be promoted as master whenever you need it, ain&rsquo;t that cool!</p>

<p>These are the steps to create a replica:</p>

<ol>
<li><p>In the master just enter the next line:</p>

<p> <em>ipa-replica-prepare your_ipa_server_name</em></p>

<p>it creates a file named replica-info-your_ipa_server_name</p></li>
<li><p> In the slave:</p>

<p><em>ipa-replica-install replica-info-your_ipa_server_name &ndash;setup-ca &ndash;setup-dns &ndash;no-forwarders &ndash;skip-conncheck</em></p>

<p><strong>&ndash;setup-ca.-</strong> put this parameter in order to promote your replica as master when you need it. This parameter is optional, but take into account that when you want to promote your replica, you will need to install the CA information at that moment.</p>

<p><strong>&ndash;setup-dns.-</strong> this parameter is to clone your DNS configuration to your replica as well as replicate DNS information.</p></li>
</ol>


<p>And that?s it!</p>

<p>In the next diagram we show you a brief overview about schema replication in Segundamano:</p>

<p><img src="http://backstage.segundamano.mx/images/freeipa_replication.jpg" alt="FreeIPA replication" /></p>

<h2><strong>Integration with OKTA</strong></h2>

<p>OKTA is a tool that once is integrated with an LDAP Directory allows us to have a centralized access and management of a predefined set of commercial applications such as Gmail, Google Apps, Slack, Github, Sales Force, etc. In Segundamano we had integrated FreeIPA as our LDAP Server with OKTA. After this we can login in all internal and external systems handled by OKTA with the same login information (user and password).</p>

<p>With this implementation we are able to manage our user database more efficiently and we can create policies which will rule the permissions to access the applications and systems based on the profiles of every person in the company.</p>

<p><strong>Advantages</strong></p>

<ul>
<li><p>A single access with the same password (SSO Single Sign ON)</p></li>
<li><p>Automatic Updates App of Web Apps (OKTA is responsible for updating all)</p></li>
<li><p>Integration of new tools (OKTA is continually developing new applications compatibility)</p></li>
</ul>


<p><img src="http://backstage.segundamano.mx/images/okta_apps.png" alt="OKTA apps" /></p>

<p><strong>Implementation</strong></p>

<p>Our experience implementing LDAP OKTA made us realize we must know beforehand the main parameters of our LDAP deployment</p>

<ul>
<li>orgUrl</li>
<li>ldaphost</li>
<li>LDAPPort</li>
<li>ldapAdminDN</li>
<li>ldapAdminPassword</li>
<li>baseDN</li>
</ul>


<p>It is essential to have a basic knowledge about LDAP (FreeIPA) for reliable deployment. Also we need to install an agent in FreeIPA server so you can have communication with OKTA (cloud), once the configuration is done in OKTA you have to log in at portal as an administrator.</p>

<p>The OKTA configuration is managed from FreeIPA (OKTA uses and agent to check everything is setup correctly), this is a simple way to describe the implementation of OKTA with LDAP.</p>

<p>Once the OKTA agent is configured in our LDAP it was necessary to enable part of SSO in Google Apps to access all our applications in Google, this will instruct Google to use the OKTA credentials instead of their own.</p>

<p>With this technique we avoid having several accesses and passwords for different tools.</p>

<p><img src="http://backstage.segundamano.mx/images/okta_architecture.png" alt="OKTA architecture" /></p>

<h2><strong>Expected Results</strong></h2>

<p>With this solution we should be able to manage easily and without waste of time the everyday applications by having a central point of control and giving the users a portal where they can access the apps.</p>

<p>This needs applications supporting OKTA integration, at the moment we&rsquo;re handling through this system the following apps:</p>

<ul>
<li>Google Aps</li>
<li>Gmail</li>
<li>Google Drive</li>
<li>Google Calendar</li>
<li>Slack</li>
<li>Yammer</li>
<li>Box</li>
<li>Github</li>
<li>Jira</li>
<li>Confluence</li>
<li>Sales Force (In process)</li>
</ul>


<p><img src="http://backstage.segundamano.mx/images/okta_supported_apps.png" alt="OKTA supported apps" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hackamano, Segundamano.mx's Own Hackathon Is Coming!]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/05/27/hackamano/"/>
    <updated>2015-05-27T08:26:26-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/05/27/hackamano</id>
    <content type="html"><![CDATA[<p>Segundamano.mx is organazing its very own Hackathon: Hackamano 2015.
The event will take place at the ITAM University campus in Mexico City
the 20 &amp; 21 June 2015.</p>

<p>The idea is that anyone can participate and be part of a team, with
a very ambitious goal in mind: develop solution to today&rsquo;s problems in
local communities.</p>

<p>This can span from improving access to education, water supply, waste
management, public transportation or whatever you think it makes sense
in your social context.</p>

<p>This will be the only rule, everything else will be limited by your own
imagination!</p>

<p>To get more information about the event, please refer to the <a href="http://hackamano.segundamano.mx">official page</a>. The registration page is also up and running <a href="http://bit.ly/hackamano-registro">here</a>. Places are limited, go register now!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Follow Us on GitHub!]]></title>
    <link href="http://backstage.segundamano.mx/blog/2014/10/21/follow-us-on-github/"/>
    <updated>2014-10-21T10:39:11-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2014/10/21/follow-us-on-github</id>
    <content type="html"><![CDATA[<p>Segundamano.mx is now on GitHub. Follow our <a href="https://github.com/SegundamanoMX">Organization Page</a>.</p>

<p>The first repository that we have publised is this blog itself, whose source code
will be available at <a href="https://github.com/SegundamanoMX/octopress">this location</a>
and we hope that with time we&rsquo;ll be able to publish more bits and pieces, including contributions
to third party FLOSS, our own software, and some challenges that we&rsquo;ll tackle together with some
local communities.</p>

<p>Stay tuned, and feel free to fork and contribute back on any of our public repositories!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgresql Metrics With Logstash]]></title>
    <link href="http://backstage.segundamano.mx/blog/2014/07/07/postgresql-metrics-pipeline/"/>
    <updated>2014-07-07T08:00:59-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2014/07/07/postgresql-metrics-pipeline</id>
    <content type="html"><![CDATA[<p>I thought I&rsquo;d share our setup at <a href="http://www.segundamano.mx">SegundaMano.mx</a>
for extracting PostgreSQL metrics with <a href="http://www.logstash.net">Logstash</a> to
push to graphs. We&rsquo;re planning on upgrading our database cluster from an older
PostgreSQL version, and we want to know what potential performance bottlenecks
we have before and after upgrading</p>

<h2>Goals</h2>

<p>Our goals here are to answer the questions:</p>

<ul>
<li>Which of our queries are slow?</li>
<li>What&rsquo;s the ratio of slow queries to non-slow queries?</li>
<li>What&rsquo;s the average query time?</li>
</ul>


<p>We want these in place before we do a major overhaul of our databases,
so we can see how our upgrades perform. Graphs are always better than
&ldquo;well it feels a lot better&rdquo;</p>

<p>Our choice of tooling landed on <a href="http://logstash.net">Logstash</a>
parsing the CSV log, extracting data to push to Statsd/Graphite and to
Elasticsearch/Kibana via RabbitMQ. In addition to Logstash,
other tools can extract data from the CSV log, for example
<a href="http://dalibo.github.io/pgbadger">pgBadger</a> and PostgreSQL
itself.</p>

<p>From the CSV log, we extract the query duration of all queries and
push this to statsd. We also fire off a counter for each query, and a
separate for each slow query.</p>

<h2>Parts involved</h2>

<p>We broke this down into 4 different roles - this might change at some
later point but it&rsquo;s a start. I drew a little picture with the various
components - the dotted lines denote host boundaries</p>

<p><img src="http://backstage.segundamano.mx/images/logstash_pipeline.png"></p>

<h3>Database servers</h3>

<ul>
<li>Postgres with CSV logging</li>
<li>Logstash</li>
<li>Statsd</li>
</ul>


<h3>RabbitMQ servers</h3>

<ul>
<li>RabbitMQ in a cluster</li>
</ul>


<h3>Graphite server</h3>

<ul>
<li>Graphite (carbon-cache, whisper, graphite-web)</li>
</ul>


<h3>Elasticsearch server:</h3>

<ul>
<li>Logstash indexer</li>
<li>Elasticsearch</li>
<li>Kibana</li>
<li>Nginx</li>
</ul>


<h2>The pipeline components</h2>

<h2>Logstash</h2>

<p><a href="http://logstash.net">Logstash</a> is basically a Unix pipe on steroids.
It&rsquo;s a JRuby project with a lot of input, filter, and output plugins
hosted by Elasticsearch. It really shines when connected with
ElasticSearch and Kibana.</p>

<p>Usually what&rsquo;s done is a logstash &ldquo;agent&rdquo; instance runs on each server
that parses the logs, mangles them into your correct format, and push
them to a central broker. Then an &ldquo;indexer&rdquo; instance pulls them off
the broker and indexes them into ElasticSearch. This decouples the
instances a bit, allowing you to firewall off the ES instance from the
servers, or use a lightweight shipper agent lacking ES support (such
as <a href="http://beaver.readthedocs.org/en/latest/">Beaver</a>) to ship
entries.</p>

<p>As for brokers, the most common are Redis and RabbitMQ. The easiest to
use is Redis, using a queue or a channel. Unfortunately this doesn&rsquo;t
offer the routing possibilites of RabbitMQ, or the ability to see
events coming over in real time with a script performing <code>tail -f</code>
duties - with Redis you can see the firehose with <code>MONITOR</code> and that&rsquo;s it.</p>

<h2>RabbitMQ</h2>

<p><a href="http://www.rabbitmq.org">RabbitMQ</a> is an open source AMQP broker. We
have a separate Vhost for logstash, and separate users for indexer and
publisher.</p>

<p>In the vhost there&rsquo;s a persistent <code>topic</code> exchange. This means that we
can shut down either the producer or consumer without losing events on
the floor - they get queued up until the indexer comes alive again.
For availability we run a cluster over several hosts using a virtual
IP with keepalived for the clients to connect to.</p>

<h2>Statsd</h2>

<p><a href="https://github.com/etsy/statsd/">Statsd</a> is a project from Etsy. You
shove metrics to it (types available
<a href="https://github.com/etsy/statsd/blob/master/docs/metric_types.md">here</a>),
and it handles the per-timeunit bucketing, mean/std/upper90 of timers
et c and ships data to Graphite. Logstash has this as well in the
<a href="http://logstash.net/docs/1.4.1/filters/metrics">metrics</a> filter, but
it doesn&rsquo;t behave as well as I&rsquo;d like.</p>

<p>We use Vimeo&rsquo;s
<a href="https://github.com/vimeo/statsdaemon">go implementation</a> instead of
Etsy&rsquo;s NodeJS version. The main reasoning being go projects compile to
a static binary, simplifying deploys (since we don&rsquo;t use NodeJS in
production). I built a
<a href="https://gist.github.com/lflux/934eff42924e276c8673">RPM specfile</a> to
build an RPM of this.</p>

<h2>Graphite</h2>

<p><a href="http://graphite.readthedocs.org/">Graphite</a> is a graphing system
comprising several parts. Carbon-cache receives the metrics and stores
them in Whisper. Whisper is a timeseries database that doesn&rsquo;t lose resolution,
like RRD does. Graphite-web is a Django project that handles the user
interface and rendering.</p>

<p>We use PostgreSQL as the storage backend instead of
sqlite after reading <a href="http://obfuscurity.com/2013/12/Why-You-Shouldnt-use-SQLite-with-Graphite">this blog post</a></p>

<h2>Elasticsearch / kibana</h2>

<p>Logstash publishes the events into
<a href="http://www.elasticsearch.org">ElasticSearch</a>, with one index per
day. <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a> is a
HTML/JS frontend to Elasticsearch for viewing the log data. The nice
thing about Kibana is that it&rsquo;s really easy to search in the data and
randomly play around with different queries - it starts off with all
events and has nice breakdowns of the values of each. At my home
company, we use Kibana to make sense of the errors coming off our app
servers and have caught a lot of interesting bugs from randomly
playing around with the queries.</p>

<h2>Results</h2>

<p>A few hours after we enabled the whole pipeline, we already could use
the Kibana interface to spot slow queries, specifically two major
offenders that we could clear up with one index on a table.</p>

<p>Kibana slow query count:
<img src="http://backstage.segundamano.mx/images/kibana_slows.png"></p>

<p>Graphite queries vs slow queries
<img src="http://backstage.segundamano.mx/images/query_count.png"></p>

<p>Graphite query time
<img src="http://backstage.segundamano.mx/images/query_time.png"></p>

<!-- more -->


<h2>Configuration Details</h2>

<h2>On the PostgreSQL server</h2>

<h3>postgresql.conf</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Log both to syslog and CSV. A .log file will be created, but it will
</span><span class='line'>be empty.
</span><span class='line'>log_destination = 'syslog,csvlog'
</span><span class='line'>log_filename = 'postgresql-%Y-%m-%d.log'
</span><span class='line'># Log duration of all statements. 
</span><span class='line'># Log full statement of any that takes  more than 2 seconds.
</span><span class='line'>log_duration = on
</span><span class='line'>log_min_duration_statement = 2000
</span><span class='line'>log_statement = 'none'
</span><span class='line'># If you log in a locale such as es_MX, logstash might not parse the CSV correctly
</span><span class='line'>lc_messages = 'C'</span></code></pre></td></tr></table></div></figure>


<h3>Logstash shipper</h3>

<figure class='code'><figcaption><span>logstash-postgresql-shipper.conf</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>        file {
</span><span class='line'>                "path" =&gt; "/path/to/pg_log/*.csv"
</span><span class='line'>                "sincedb_path" =&gt; "/path/to/pg_log/sincedb_pgsql"
</span><span class='line'># fix up multiple lines in log output into one entry              
</span><span class='line'>           codec =&gt; multiline {
</span><span class='line'>                   pattern =&gt; "^%{TIMESTAMP_ISO8601}.*"
</span><span class='line'>                   what =&gt; previous
</span><span class='line'>                   negate =&gt; true
</span><span class='line'>           }
</span><span class='line'>        }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'># See http://www.postgresql.org/docs/9.3/interactive/runtime-config-logging.html#RUNTIME-CONFIG-LOGGING-CSVLOG
</span><span class='line'>        csv {
</span><span class='line'> columns =&gt; [  "log_time", "user_name", "database_name", "process_id", "connection_from", "session_id", "session_line_num", "command_tag", "session_start_time", "virtual_transaction_id", "transaction_id", "error_severity", "sql_state_code", "sql_message", "detail", "hint", "internal_query", "internal_query_pos", "context", "query", "query_pos", "location"]
</span><span class='line'>        }
</span><span class='line'>  mutate {
</span><span class='line'>         gsub =&gt; [ "sql_message", "[\n\t]+", " "]
</span><span class='line'>  }
</span><span class='line'># use timestamp from log file  
</span><span class='line'>  date {
</span><span class='line'>        #2014-05-22 17:02:35.069 CDT
</span><span class='line'>        match =&gt; ["log_time", "YYYY-MM-dd HH:mm:ss.SSS z"]
</span><span class='line'>}
</span><span class='line'>  grok {
</span><span class='line'>    match =&gt; ["sql_message", "duration: %{DATA:duration:int} ms"]
</span><span class='line'>    tag_on_failure =&gt; []
</span><span class='line'>        add_tag =&gt; "sql_message"
</span><span class='line'>  }
</span><span class='line'># See postgres configuration - a message with 'statement: ' is a slow query
</span><span class='line'>  grok  {
</span><span class='line'>        match =&gt;["sql_message", "statement: %{GREEDYDATA:statement}"]
</span><span class='line'>        tag_on_failure =&gt; []
</span><span class='line'>        add_tag =&gt; "slow_statement"
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'># Increase hitcounter when we see a slow statement
</span><span class='line'>  if "slow_statement" in [tags] {
</span><span class='line'>        statsd {
</span><span class='line'>                increment =&gt; "postgresql.slow_queries"
</span><span class='line'>        }
</span><span class='line'>  }
</span><span class='line'>  # increase hitcounter for all queries
</span><span class='line'>  # send timing metrics for queries
</span><span class='line'>  if "sql_message" in [tags] {
</span><span class='line'>          statsd {
</span><span class='line'>                timing =&gt; {"query_duration" =&gt; "%{duration}"}
</span><span class='line'>                increment =&gt; "postgresql.queries"
</span><span class='line'>          }
</span><span class='line'>  }
</span><span class='line'>  # Ship off all to rabbitmq
</span><span class='line'>  rabbitmq {
</span><span class='line'>    'durable' =&gt; true
</span><span class='line'>    'exchange' =&gt; 'logstash'
</span><span class='line'>    'exchange_type' =&gt; 'topic'
</span><span class='line'>    'host' =&gt;  "server"
</span><span class='line'>  #note - replace type and host manually since as of writing this isn't replaced like it should be
</span><span class='line'>    'key' =&gt; 'logstash.%{type}.%{host}'
</span><span class='line'>    'password' =&gt; "password"
</span><span class='line'>    'persistent' =&gt; true
</span><span class='line'>    'user' =&gt; 'shipper'
</span><span class='line'>    'vhost' =&gt; 'logstash'
</span><span class='line'>    'workers' =&gt; 4
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>Statsdaemon</h3>

<p>Not much to this here - just run statsdaemon and edit the configuration file <code>/etc/statsdaemon.ini</code> and set the correct graphite host.</p>

<h2>RabbitMQ</h2>

<p>Set up rabbitmq with a Vhost for logstash, create users indexer and
shipper that can read and write to the logstash vhost. Before starting
the shipper, start the indexer which will automatically create the
binding from the exchange to the queue.</p>

<h2>ElasticSearch/Indexers</h2>

<h3>ElasticSearch</h3>

<p>Install elasticsearch with a fair bit of storage.</p>

<figure class='code'><figcaption><span>/etc/elasticsearch/elasticsearch.yml</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">cluster.name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">elasticsearch</span>
</span><span class='line'><span class="l-Scalar-Plain">path.conf</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/etc/elasticsearch</span>
</span><span class='line'><span class="l-Scalar-Plain">path.data</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/opt/logs/elasticsearch/data</span>
</span><span class='line'><span class="l-Scalar-Plain">path.logs</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/opt/logs/elasticsearch/logs</span>
</span><span class='line'><span class="l-Scalar-Plain">discovery.zen.minimum_master_nodes</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">1</span>
</span><span class='line'><span class="l-Scalar-Plain">discovery.zen.ping.multicast.enabled</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Logstash Indexer</h3>

<p>Configure to pull events off RabbitMQ and index them into Elasticsearch</p>

<figure class='code'><figcaption><span>/opt/logstash/server/etc/conf.d/logstash.conf</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>
</span><span class='line'>      rabbitmq {
</span><span class='line'>        'auto_delete' =&gt; 'false'
</span><span class='line'>        'codec' =&gt; 'json'
</span><span class='line'>        'debug' =&gt; true
</span><span class='line'>        'durable' =&gt; true
</span><span class='line'>        'exchange' =&gt; 'logstash'
</span><span class='line'>        'exclusive' =&gt; false
</span><span class='line'>        'host' =&gt; 'host'
</span><span class='line'>        'key' =&gt; 'logstash.#'
</span><span class='line'>        'password' =&gt; 'passwd'
</span><span class='line'>        'queue' =&gt; 'logstash-indexer'
</span><span class='line'>        'user' =&gt; 'indexer'
</span><span class='line'>        'vhost' =&gt; 'logstash'
</span><span class='line'>      }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch { host =&gt; "host"
</span><span class='line'>        cluster =&gt; "logstash"
</span><span class='line'>        protocol =&gt; "http"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>Kibana</h3>

<p>Install kibana from <a href="https://github.com/elasticsearch/kibana">git master</a> into the directory
where you want it served from.</p>

<h3>Nginx</h3>

<p>Set up Nginx to show kibana HTML and to proxy requests to Elasticsearch</p>

<figure class='code'><figcaption><span>/etc/nginx/sites-enabled/kibana</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class='nginx'><span class='line'><span class="k">server</span> <span class="p">{</span>
</span><span class='line'>  <span class="kn">listen</span>                <span class="n">host</span><span class="p">:</span><span class="mi">80</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="kn">server_name</span>           <span class="s">host</span><span class="p">;</span>
</span><span class='line'>  <span class="kn">access_log</span>            <span class="s">/var/log/nginx/kibana.log</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="kn">location</span> <span class="s">/</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">root</span>  <span class="s">/opt/kibana/current/src</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">index</span>  <span class="s">index.html</span>  <span class="s">index.htm</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_aliases$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_aliases$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_nodes$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_search$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_mapping$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_cluster/health$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># password protected end points</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/kibana-int/dashboard/.*$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/kibana-int/temp.*$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
</feed>
