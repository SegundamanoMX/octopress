<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Segundamano's Backstage]]></title>
  <link href="http://backstage.segundamano.mx/atom.xml" rel="self"/>
  <link href="http://backstage.segundamano.mx/"/>
  <updated>2016-01-15T19:01:21-06:00</updated>
  <id>http://backstage.segundamano.mx/</id>
  <author>
    <name><![CDATA[Segundamano.mx Tech Team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Test Automation Tips]]></title>
    <link href="http://backstage.segundamano.mx/blog/2016/01/14/test-automation-tips/"/>
    <updated>2016-01-14T15:48:56-06:00</updated>
    <id>http://backstage.segundamano.mx/blog/2016/01/14/test-automation-tips</id>
    <content type="html"><![CDATA[<p>Over my 15 year experience in the field of Software Quality Assurance, at almost all my jobs and projects, I was required to use some type of automation tool/framework in order to improve quality and time to complete the different test phases.  Based on this experience I have learned the importance of automation testing and I learned a few things along the way. I would like to share a few tips for anyone interested in starting or improving test automation in their projects.</p>

<h2>Follow one strategy for your overall testing process</h2>

<p>When you have a need to start automating or improving your testing, it is highly recommended that you follow an overall test strategy in order to clearly determine what needs and can be automated. Know exactly what you want to accomplish and how you can do that.</p>

<p>For example, for agile testing there is a recommended approach based on the four agile testing quadrants. There are some good articles on the internet and even books written about it. At <a href="http://www.segundamano.mx">segundamano.mx</a> we base our strategy on these four agile testing quadrants following the recommendations provided in the book <em>Agile Testing</em> by Lisa Crispin and Janet Gregory, highly recommended for anyone following Agile methodologies.</p>

<p>Following this approach, it is very clear to us what should be automated, who is responsible for it, what phase might take priority and how we can distribute our automated testing.</p>

<p><img src="http://backstage.segundamano.mx/images/agile_four_quadrants.png"></p>

<h2>Never compare Manual vs Automated Testing</h2>

<p>They serve two different purposes and depending on the project and technology to be tested you will use one or the other or in most cases both.</p>

<p>My first project that involved test automation was to test a wireless switch and it mainly consisted in injecting messages to the switch and expected a response back (testing protocols). In this case, 98% of our testing was done with automated scripts (using TCL) which made a lot of sense since there was no direct interaction with the users. However, later on when I was testing cellular phones it made a lot of sense to do a lot of manual testing since we wanted to make sure the user experience was considered.</p>

<p>Consider that some testing such as exploratory testing and anything related to User Experience should probably be done manually since it requires a human mind to identify issues/bugs related to this area.</p>

<h2>Not everything can be automated</h2>

<p>Somehow related to my previous point but I wanted to be more specific to determine how much of your testing should automated. While testing cellular phones at Motorola I was involved in a project where the goal was to automate more than 90% of our manual test cases. Unfortunately this could not be accomplished mainly for the following reason:</p>

<p>The User Interface of the different devices was constantly changing, the flows were changing and although we tried to modify the internal tool to mitigate these problems it was just impossible to keep up with the changes.</p>

<p>When thinking about test automation make sure you make a good initial analysis involving the right people. Consider the features to test, are they something to stay for a long period of time or temporary? are they stable enough or not, how often will they change? Does it make sense to automate it in regards to time, effort and subsequent use? Automate what makes sense and consider the limitations automation might have.</p>

<h2>Communication between QA and Development is key for success</h2>

<p>Communication problems between testers and developers are a thing of the past, well, it&rsquo;s a nice thought but we still have to work on it. A good start can be working together to automate all testing phases. When I joined <a href="http://www.segundamano.mx">segundamano.mx</a> one of my first assignments was to investigate how to migrate (and actually do it) our regression scripts from selenium to rspec/capybara. These were test scripts mainly managed by the developers used in our continuous integration setup.</p>

<p>During the process, there was a lot of involvement from the developers in regards to learning the previous testing framework, learning the new capybara tool, using some tools to automatically translate the scripts and actually helping with the migration. At the end of the project every developer and tester in the team was involved in migrating at least one script from selenium to capybara.</p>

<p>The lessons learned:</p>

<ul>
<li>Get developers involved in all phases of testing.</li>
<li>Clearly define who is responsible for the different activities related to automation, from unit/component testing to functional and manual testing.</li>
<li>Consider the opinion of developers when selecting a tool to automate, specially if they will be using the tool.</li>
<li>As a tester, understand what is getting automated by the developers in order to know what you need to automate and test at later phases.</li>
</ul>


<h2>Select the right tool </h2>

<p>Choosing the wrong tool can cause a lot of pain and frustration but fortunately we have lots of options. At <a href="http://www.segundamano.mx">segundamano.mx</a> we use rspec/capybara, go, selenium and most recently intern. Each one for a different user/team/technology and with a different purpose. You will need to define first what are your needs, who will be using the tool,  during which phase  it will be used, for which type of testing (eg. unit vs functional).</p>

<p>Take your time to decide and don&rsquo;t be afraid to try them, lots of them are open source and the ones that are not usually have a free trial. Get the developers involved, they always provide good feedback. Also, consider how big is the community supporting the test tool/framework, the bigger the community the better since you will be able to find answers to your questions faster.</p>

<h2>Automation of New Features vs Regression Scenarios</h2>

<p>So you to the point where have a good set of regression test cases but continue to add lots of new functionality, what should you do about this new functionality?</p>

<p>When new functionality is added you need to think which scenarios have to be automated in order to add them as part of the regression. A happy path of the new functionality should always be part of the regression plus a set of negative test cases. The number of test cases will be based on the size and importance of the functionality but the main idea is to verify that the new functionality continues to work one or more years down the road without any problems.</p>

<h2>The sooner you find bugs the better</h2>

<p>Finding a bug during unit testing will take a developer probably just a few minutes to fix. Finding a bug in production will make everyone&rsquo;s life miserable, time for fixing, testing and releasing a bug fix will be needed&hellip;. time means money.  No matter how you look at it, the sooner you find bugs the better for everyone. When deciding which test phase to automate and how much testing should be done for each phase always consider this.</p>

<p>I was not trying to provide a complete solution to how and when automated testing should be done, I just wanted to provide some tips based on my experience and share it with you. Hopefully you found it informative. Please feel free to provide any comments and share your own experiences.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How We Backed Our Amazon S3 Bucket Within S3]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/07/02/backup-your-s3-bucket/"/>
    <updated>2015-07-02T09:00:00-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/07/02/backup-your-s3-bucket</id>
    <content type="html"><![CDATA[<p><strong>TL;DR:</strong>
Here at <a href="http://www.segundamano.mx/">segundamano.mx</a> we store a fair amount of our content in <a href="http://aws.amazon.com/s3/">Amazon S3</a>.
Backup and restore of our S3 content is a must.
We did not find a system that does the job.
We tried <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html">cross-region replication</a> and <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html">versioning</a>, and although neither is proper backup and restore system, they are building blocks for a proper system.
Thus, we developed <a href="https://github.com/SegundamanoMX/backup-my-bucket">backup-my-bucket</a>, an open source tool that complements cross-region replication and versioning to form a backup and restore system for S3 buckets.</p>

<h2>You should backup your bucket</h2>

<p>When you run a site with a huge amount of content, delegating administration of storage to a cloud service is something you should consider.
Here at <a href="http://www.segundamano.mx/">segundamano.mx</a> we choose <a href="http://aws.amazon.com/s3/">Amazon S3</a>.</p>

<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig1.png">
<em><strong>Figure 1:</strong> Our site writes and reads some content from a master bucket.</em></p>

<p>Once you start storing content on Amazon S3 (but really, hopefully before), you will start worrying about reliability of your site.
My colleages and I are commited to designing and developing systems that are highly-available and fault tolerant.
But despite your efforts, it is always a good idea to have a disaster recovery plan for your S3 buckets.</p>

<p>We consider that a good disaster recovery plan for Amazon S3 considers system and human errors.
A system error is the loss of a file due to data corruption or the deletion of a set of files due to a bug in our code.
A human error is the deletion of a set of files, for example due to <a href="http://docs.aws.amazon.com/cli/latest/reference/s3/rm.html">aws s3 rm s3://master-bucket/cozumel.jpg</a>.
For data corruption, you could argue that <a href="http://aws.amazon.com/s3/faqs/#data-protection_anchor"><em>Amazon&rsquo;s 99.999999999%</em> durability</a> is enough and <a href="http://stackoverflow.com/a/17839589">forget about that</a>.
For the accidental deletion of files due to system or human error, you could consider that copying by cross-region replication or <a href="https://aws.amazon.com/blogs/aws/archive-s3-to-glacier/">glacier storage</a> is enough.
But truth is that, when you go into production, you won&rsquo;t feel very confident.</p>

<p>Backup is an essential part of an effective disaster recovery plan.
We consider that a system that does the job let&rsquo;s you:</p>

<ol>
<li>Backup your master bucket.</li>
<li>Restore your master bucket to a given point in time.</li>
<li>Manage your backups.</li>
</ol>


<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig2.png">
<em><strong>Figure 2:</strong> Abstract architecture of a backup and restore system. Backup and restore operate on the master and slave buckets. The system should offer a management console, run from a control machine.</em></p>

<h2>Approaches to backup</h2>

<p>A first approach to backup is to store backups in your local data center.
This is not what we want because it defeats the purpose of using S3.
We want to store our master bucket in another S3 bucket.</p>

<p>For backing up master bucket in a slave bucket, you can <a href="http://serverfault.com/a/239722">copy the contents from master to slave</a>.
This however takes a lot of time and managing versions by hand is difficult.</p>

<p>One could be tempted to avoid copying altogether by versioning.</p>

<blockquote><p><strong><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html">Versioning</a>:</strong> for a given key and key operation, preserve the current
  content of the key after the operation is applied on key. Example:
  preserve the content of key s3://master-bucket/cozumel.jpg after
  <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/DeletingObjectVersions.html">deleting</a> the key.</p></blockquote>

<p>However, versioning alone does not offer a way to manage backups.
Moreover, versioning does not isolate versions from operations applied to master bucket and it is perfectly possible to delete <a href="http://boulderapps.co/post/remove-all-versions-from-s3-bucket-using-aws-tools">all versions of a set of files</a>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">echo</span> <span class="s1">&#39;#!/bin/bash&#39;</span> &gt; deleteBucketScript.sh
</span><span class='line'>
</span><span class='line'>aws --output text s3api list-object-versions --bucket master-bucket <span class="p">|</span> <span class="se">\</span>
</span><span class='line'>grep -E <span class="s2">&quot;^VERSIONS&quot;</span> <span class="p">|</span> <span class="se">\</span>
</span><span class='line'>awk <span class="s1">&#39;{print &quot;aws s3api delete-object --bucket master-bucket --key &quot;$4&quot; --version-id &quot;$8&quot;;&quot;}&#39;</span> &gt;&gt; deleteBucketScript.sh
</span><span class='line'>
</span><span class='line'>. deleteBucketScript.sh
</span></code></pre></td></tr></table></div></figure>


<p>Cross-region replication sounds more like what we need.</p>

<blockquote><p><strong><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html">Cross-region replication</a>:</strong> for given buckets master &amp; slave, and
   given key operation, apply operation to bucket slave after
   operation is applied to master. Example: upload file cozumel.jpg
   to bucket slave after I upload file cozumel.jpg to bucket master.</p></blockquote>

<p>Cross-region replication has two advantages:
(1) replicates creation and deletion of files, and
(2) restricts the location of the master and the slave so that they are in different locations.
However, restoration and management of versions is still difficult and unpractical</p>

<h2>Our solution</h2>

<p>We approached problem by complementing cross-region replication and versioning with backup management and a restore operation.
We implemented managemenent and restore in the command line tool <code>backup-my-bucket</code>.
We wrote <code>backup-my-bucket</code> in <a href="http://golang.org/">Go</a> and made it <a href="https://github.com/SegundamanoMX/backup-my-bucket">open source</a>.
We take advantage of it&rsquo;s facilities for writting concurrent code, in particular <a href="https://gobyexample.com/goroutines">goroutines</a> and <a href="https://gobyexample.com/channels">channels</a>.
With <code>backup-my-bucket</code> you backup and restore master into slave by means of a control machine.
There is no restriction on where your control machine is located, e.g. your data center or Amazon EC2.</p>

<h2>Backup</h2>

<p>When we backup we create a restoration point.
A restoration point is a copy of the contents of master at a given point in time.
We create restoration points in two steps:
(1) copy contents of master to slave, and
(2) snapshot contents of slave.
Each restoration point consists of a snapshot and a set of files.</p>

<p>For the copy, we apply cross-region replication.
Cross-region replication continually copies contents of master to slave.</p>

<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig3.png">
<em><strong>Figure 3:</strong> We use cross-Region replication to continually replicate onto slave bucket each operation applied to master bucket.</em></p>

<p>With this approach we can make backups faster in the sense that we do not wait several hours for a copy operation to bring all data from master to slave
Instead, by copying keys as soon as they are modified we obtain the latest version of slave in master at any given time.
Moreover, <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr-what-is-isnot-replicated.html">cross-region replication does not replicate version deletes</a>, thus protects from malicious deletion.</p>

<p>Cross-region replication requires enabling versioning on the slave bucket.
Thus, every time we operate on a key in the slave bucket, versioning takes care of creating a version of that key.</p>

<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig4.png">
<em><strong>Figure 4:</strong> Every time cross-region replication applies a change to slave bucket, versioning stores corresponding versions in slave bucket. The write to file <code>A</code> creates version <code>A3</code>, the delete of file <code>C</code> creates delete marker <code>DEL</code>.</em></p>

<p>Versioning releases you from implementing a repository of versions.
You only have to care about removing versions when they become obsolete.</p>

<p>The snapshot of the slave bucket is an index of the contents of slave.
Creation of the snapshot happens when you run command <code>backup-my-bucket snapshot</code>.</p>

<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig5.png">
<em><strong>Figure 5:</strong> Snapshot of the slave bucket proceeds by creating an index of the current versions of the keys in slave.</em></p>

<p>The command saves the snapshot in the control machine.
The snapshot may be compressed.
A particular challenge to taking snapshots of S3 buckets is that for each directory, S3 API will list files in batches of 1000.
We approach the problem by <a href="https://github.com/SegundamanoMX/backup-my-bucket/blob/0.1.0/snapshot/snapshot.go">exploring the slave bucket breadth-first</a>.
For each new directory found we spawn a new worker goroutine, thus distributing queries to S3 API and making a more efficient use of resources than a depth-first approach.
The number of workers is configurable.
We usually take less than an hour to process c.a. 15 million keys in a flat directory tree.
Of course asking for more workers than the count of directories in your directory tree will bring no gain in speed.</p>

<h2>Restore</h2>

<p>For a given snapshot, restore is the copy of the corresponding versions from slave bucket to master bucket.
Restore happens when you run command <code>backup-my-bucket restore</code>.</p>

<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig6.png">
<strong>Figure 6:</strong> A run of restore from slave into master is executed from the control machine. Restore proceeds by copying versions corresponding to a given snapshot.</p>

<p>Given the snapshot, the <a href="https://github.com/SegundamanoMX/backup-my-bucket/blob/0.1.0/restore/restore.go">tool</a> distributes the copy of corresponding versions amongst a configurable number of worker goroutines.
The limit to the number of workers is dictated by your system limits, resources, and your upload speed.
We usually take several hours to restore a terabyte of data.</p>

<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig7.png">
<em><strong>Figure 7:</strong> We distribute restore operation amongst several workers, each copies a version key from slave to master at a time.</em></p>

<h2>Manage</h2>

<p>Management of restoration points consists in periodically running command <code>backup-my-bucket gc</code>.
The command will remove all obsolete restoration points.
That includes the snapshot file and all corresponding versions in the slave bucket.
A restoration point is obsolete when it is older than the retention policy.
If there are not enough restoration points to meet the minimum redundancy policy, the command will not remove any restoration point.</p>

<p><img src="http://backstage.segundamano.mx/images/backup-your-s3-bucket/fig8.png">
<em><strong>Figure 8:</strong> A run of <code>gc</code> removes all the versions corresponding to an obsolete restoration point.</em></p>

<h2>Future work</h2>

<p>Currently, <code>backup-my-bucket</code> is in beta stage.
For future versions, we would love to have the following features that we may or may not develop.</p>

<ol>
<li>Copy files from master to slave during creation of restoration
points. Instead, we currently rely on cross region replication to copy
contents in advance.</li>
<li>Compress contents of restoration points. We already compress snapshot files, so nothing to do there.</li>
<li>Do recovery by fall back. That is, promote slave to master and move restoration points to new slave.</li>
</ol>


<p>We encourage people to try <code>backup-my-bucket</code> out and submit pull requests.
And, of course, we are <a href="http://backstage.segundamano.mx/work-with-us/">hiring</a>, so drop us a line.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous Delivery]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/07/01/continuous-delivery/"/>
    <updated>2015-07-01T16:08:49-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/07/01/continuous-delivery</id>
    <content type="html"><![CDATA[<p>Continuous Delivery is a software development approach in which teams keep producing code in short cycles and ensure that the software can be reliably and released at any time.</p>

<p>What&rsquo;s Blue/Green deploy strategy? There&rsquo;s a lot of different opinions and definitions, for us we see it as a way to avoid being offline if something goes wrong with a new deploy of functionality (and being shouted at, or fired).</p>

<p>The basic premise is to duplicate your production stack in order to have one with the most recent software installed and the other with the last stable version (and proved in production). This way if something happens with the new deploy you can always perform a rollback as fast as you can switch traffic to the old stack.</p>

<p>About the &ldquo;blue&rdquo; and &ldquo;green&rdquo; concepts, sometimes the new version is called blue while green is the fallback; for some people is the other way around. That&rsquo;s why we&rsquo;ll use <strong>fallback</strong> to describe the &ldquo;old, stable&rdquo; version of the system while we&rsquo;ll use <strong>canary</strong> to describe a new, shining and hopefully better version of it.</p>

<p>This brings another concept: canary.</p>

<p>It&rsquo;s simply to use a chunk of stack for deploying new versions. This way you&rsquo;ll have a canary in your stack in which validation over production will be performed. It&rsquo;s refered as canary for the real-live canaries used in mines to &ldquo;supervise&rdquo; the quality of the air, if the canary dies (because the air is rare) the miners could have time to be saved.</p>

<p>The canary part of the stack gradually becomes bigger and bigger and at some point your whole stack is canary. This way you tested progressively in production a new feature without the risk of putting your site offline.</p>

<h2><strong>Current Situation</strong></h2>

<p>Currently we are working with a development methodology in which new features are delivered to production twice a month and usually with a lot of features. This practice increased the complexity to resolve a problem: as many features you develop is as many components you have to troubleshoot in case of a problem and you don&rsquo;t know which one caused the problem.</p>

<p>When a new feature is developed it has to wait until the next release (two weeks) to be in production, this makes our delivery process very slow.</p>

<p>In the past we had to install all the new developed packages manually at our infrastructure, this means enter in every single server involved for the release process, now we are using a super cool tool which is called SaltStack as a configuration and orchestration management system, this tool allows us to install automatically and quickly new version of packages.</p>

<p>If you are familiar with Puppet or Chef you would know a little bit more about what we are talking about, if not, you can search by your own the configuration and remote execution management tool that suites your needs.</p>

<p>The Segundamano infrastructure seeing from the point of view of the interaction between Front Servers (www, php, nginx, varnish, etc) and the End-User&rsquo;s request, begins with the firewalls as a first level that includes security for layer 3 and 4 and which redirect (via DNAT) to a second level composed for Load Balancers with High Availability (HA) capabilities, in other words these load balancers in our case Keepalived can create a virtual IP Address (VRRP) and they have the task besides the HA to balance at layer 3 and 4 the user&rsquo;s request for the next level composed by the Front platform, currently there are two more lower levels which are the backend, and finally the Data Layer (Database.- PostgreSQL),  but they are not touched by the load balancing layer  as we can see in the next Diagram:</p>

<p><img src="http://backstage.segundamano.mx/images/as_is_infrastructure.jpg" alt="Infrastructure as is" /></p>

<h2><strong>Goals</strong></h2>

<p>Now, there are many ways to conceptualize the Blue/Green/Canary paradigm.  Depending on your needs, current situation and obviously  your infrastructure architecture.</p>

<p>Based on the idea presented in the introduction of this article and having in mind our current situation and the infrastructure that Segundamano has, we decided to represent and therefore to implement Blue/Green/Canary <em>under the concept that all frontals servers can be a potential candidate to have the role or belong to one of the Stacks mention before, this is to be a  Blue, Green or Canary server</em>.</p>

<p>Yea you are right !!, this sounds so simple, but there are some key points to have in mind in order to pass through correctly from the concept to the implementation:</p>

<ol>
<li><p>When we want to release a new potential version,  we just want to redirect a small percentage of the user&rsquo;s request in order to test functionality (<strong>Canary Stack</strong>).</p></li>
<li><p>Then if everything is going well, we are going to redirect the 100% of the user&rsquo;s request to the new potential release version of our platform.(<strong>Blue Stack</strong>)</p></li>
<li><p>But not always life is good, what if for any reason, and there are tons of them, we realized that some feature or functionality it&rsquo;s broken after we have been redirected users request to the new potential release version !!, Well, we need to redirect them back to the Stable Version we use to have (<strong>Fallback Stack</strong>)</p></li>
<li><p>We need to ensure that in the process of redirecting users request from one stack to another, the user&rsquo;s sessions are not going to be lost or closed.</p></li>
</ol>


<h2><strong>Implementation </strong></h2>

<ol>
<li><p>As we mentioned before, we must create one more stack of servers, for the new potential releases, in this case will be the Blue Stack, we already have the Green Stack  for the current release stable version, which in fact  is the only one we have today.</p></li>
<li><p>Hey wait a minute, but you have just  said  that all servers can be a candidate to be or belong for any Stack no matter if we are talking about Blue, Green or Canary, Yes you are right !!, but we need resources in order to support at least two different versions alive at same time of the platform (web, msite, etc).</p></li>
<li><p>We also need to create an extra layer of load balancers, <em>but Why another one!? if we already have the super powerful Keepalived load balancers</em>. Well, as we described earlier in this article, those  load balancers ensures HA and 3 and 4 layer load balancing (IP/TCP), but &hellip; and this is a Big But !!, we are not ensuring the users sessions layer, for tech guys this is the OSI layer 7, this means that we need to ensure that the user&rsquo;s sessions will not be lost or closed if we move them from one Stack to another.</p></li>
<li><p>So, based on the point above, we selected after some studies on the performance and usability benchmarking as well as our own know how in some tools a proxy tool which has  two main advantages:</p>

<ol>
<li><p>Layer 7 support load balancing, this is the persistence based on cookies (sessions).</p></li>
<li><p>A proxy server which can be programmable (dynamic behavior).</p></li>
</ol>
</li>
</ol>


<p>We describe in the next diagram the idea explained in the points discussed before:</p>

<p><img src="http://backstage.segundamano.mx/images/infrastructure_to_be.jpg" alt="Infrastructure to be" /></p>

<p>And talking about Load Balancing levels and layers, why not just remove the keepalived level which are the load balancers for layer 3 and 4 and just leave the new Layer 7 proxy load balancers. Well, if we want a full redundant architecture, let&rsquo;s remember that for example HAProxy or Nginx or Pen or Pound they do not support the creation of a Virtual IP Address which ensures the HA feature, and if in Segundamano already have the keepalived which has this feature, the question would be, Why not combine the features from one Load Balancer (HA and 3 and 4 layer support) and the other Load Balancer (Programmable and layer 7 cookies session support)?.</p>

<p>And &hellip; that&rsquo;s it !!, Is that all?, well, the answer is NO, let&rsquo;s remember that almost all things in life or at least in projects are about PPT.- People, Process and Tools.</p>

<p>We need a tool to help us to distribute to a big amount of servers the proper configuration that enable us to change the behavior on the fly (orchestration), for example to move a certain % of users from one stack to another and vice versa.</p>

<p>But all tools and People involved are useless if we do not define the correct processes that must be reflected on the tools we use.</p>

<p>In the next section we will talk a little bit more about the technologies we decided to use in order to achieve the Blue/Green/Canary paradigm.</p>

<p>One note, we always got confused when we tried to identify what&rsquo;s blue or what&rsquo;s green? that&rsquo;s why we stopped thinking like that and use the canary and fallback concepts; after this blue and green are only names of the stack, but as that is really, really boring we came to name it &ldquo;Topo&rdquo; and &ldquo;Gigio&rdquo; after an inside joke in the team as one of our colleagues is a lookalike of the TV program.</p>

<p>So, <strong>topo</strong> or <strong>gigio</strong> can be at anytime <strong>canary</strong> or <strong>fallback</strong> stack per service basis (but not the two at the same time).</p>

<p> <img src="http://backstage.segundamano.mx/images/topogigio.jpg" alt="Topo Gigio" /></p>

<h2><strong>Tools</strong></h2>

<p>In order to implement this solution, we had to choose the right tools that help us to achieve our goals. So we choose the following tools:</p>

<ul>
<li><strong>Salt Stack </strong></li>
</ul>


<p>Salt Stack is a python based open source configuration management and remote execution application which handle the infrastructure as code.</p>

<p>We used this tool as the server&rsquo;s brain, so we can handle our entire infrastructure from a single point, allowing us to standardize our servers configuration and to automate tasks.</p>

<p>For automation we used salt stack with jinja, that is a python templating extension that enable us to make dynamic and on the fly configuration files for the servers.</p>

<ul>
<li><strong>Keepalived</strong></li>
</ul>


<p>Keepalived is an open source Layer 3 and 4 Load Balancer Application for incoming requests between servers. The main goal of this component is to provide high availability to Linux system based Infrastructures.</p>

<p>We use Keepalived in order to provide high availability to our Proxy Load Balancers (Nginx), because of Nginx can&rsquo;t do it by itself.</p>

<ul>
<li><strong>Nginx</strong></li>
</ul>


<p>Nginx is a Free, Open Source and high performance Http server and proxy.In our experience, it has been considered the best option for handling large amount of requests.</p>

<p>We use Nginx as load balancer and as SSL Proxy. Basically all the requests that come from our end users are handled by Nginx in order to secure them via the SSL protocol.</p>

<ul>
<li><strong>Python</strong></li>
</ul>


<p>Python is a high-level programming language that has been created to emphasize the code readability and its syntax allows programmers to express concepts in fewer lines of code.</p>

<p>We use Python to create the script which orchestrate dynamically the load balancer&rsquo;s configuration (with help of SaltStack-Jinja) in order to follow with the continuous delivery process.</p>

<h2><strong>Configuration Details</strong></h2>

<p><strong>HA Proxy</strong>:</p>

<p>We start our project with HA Proxy in mind. It&rsquo;s simple, reliable and fast. Three key points we needed for balancing our traffic between stacks in order to have our &ldquo;virtual canary&rdquo;.</p>

<p>The configuration is really simple, with something like</p>

<pre><code>frontend  main *:443
    acl canary      cook(site_id) canary
    acl fallback    cook(site_id) fallback

    use_backend     canary      if canary
    use_backend     fallback    if fallback
    default_backend newuser

backend newuser
    balance roundrobin
    cookie  site_id     insert indirect maxlife 72h preserve
    server  fallback1   10.39.0.92:443 check inter 1000 cookie fallback weight 95
    server  fallback2   10.39.0.93:443 check inter 1000 cookie fallback weight 95
    server  canary1     10.39.1.90:443 check inter 1000 cookie canary weight 5
    server  canary2     10.39.1.90:443 check inter 1000 cookie canary weight 5

backend canary
    balance roundrobin
    cookie  site_id     insert indirect maxlife 72h preserve
    server  canary1     10.39.1.90:443 check inter 1000 cookie canary weight 1
    server  canary2     10.39.1.91:443 check inter 1000 cookie canary weight 1

backend fallback
    balance roundrobin
    cookie  site_id     insert indirect maxlife 72h preserve
    server  fallback1   10.39.0.92:443 check inter 1000 cookie canary weight 1
    server  fallback2   10.39.0.93:443 check inter 1000 cookie canary weight 1
</code></pre>

<p>What this does is to check if the user have a cookie site_id with values canary or fallback, if so it use the proper backend for each value of the cookie, if not we redirect it to the backend newuser and then we set the different weights of the stack directly in each server.</p>

<p>Anyway, we&rsquo;ll not get deeper into HA Proxy, as we soon found it only support session cookies. It can modify a cookie already set elsewhere, but if that cookie is missing (for example, a fresh user which have never visited our site) it will depend on the application to handle most of the logic.</p>

<p>We prefered to move to Nginx, as it can handle cookies with expiration date. It&rsquo;s not &ldquo;as simple&rdquo; as HA Proxy, but it give us a little of room to &ldquo;script&rdquo; the cookie part.</p>

<p><strong>Nginx</strong>:</p>

<ul>
<li><strong>Load balancing part:</strong></li>
</ul>


<p>We&rsquo;re using the load balancing features of Nginx, this is as &ldquo;simple&rdquo; as:</p>

<pre><code>upstream fallback {
    server          10.39.1.90 max_fails=5 fail_timeout=20;
    server          10.39.1.91 max_fails=5 fail_timeout=20;
}

upstream canary {
    server          10.39.0.93 max_fails=5 fail_timeout=20;
    server          10.39.0.92 max_fails=5 fail_timeout=20;
}
</code></pre>

<p>That give us the basic round-robin balancing of the servers in each upstream with the same weight. We set up the two stacks and then we can send traffic to them with</p>

<pre><code>location / {
    proxy_pass http://fallback$request_uri;
}
</code></pre>

<p>That force us to change configuration once we want to switch from stack (the blue/green paradigm we discuss before) and it doesn&rsquo;t allow to apply to it the concept of canary, since our way of seeing it is to make it work through balancing a portion of traffic to the new stack.</p>

<p>But in order to really replicate the features of HA Proxy we need to check if the cookie to handle the balancing between stacks is present, if not assign the new user to a stack and then proxy the connection to the right stack.</p>

<ul>
<li><strong>Cookie part:</strong></li>
</ul>


<p>For the cookie part we can use a map. As Nginx create a variable for each cookie received we only need to map it to a result. We came with the following code</p>

<pre><code>map $cookie_site_id $selected_upstream {
    default newuser;
    canary canary;
    fallback fallback;
}
</code></pre>

<p>That way, if the cookie does not exist or the value of the cookie is not &ldquo;canary&rdquo; or &ldquo;fallback&rdquo; it will return the default value of &ldquo;newuser&rdquo; in the &ldquo;selected_upstream&rdquo; variable, otherwise it will return &lsquo;canary&rsquo; or &lsquo;fallback&rsquo;.</p>

<p>After doing this we need to send the cookie to the user, for when he or she returns, it will use the same stack. For doing this the code is</p>

<pre><code>add_header Set-Cookie "site_id=$selected_upstream;Domain=.test.me;Path=/;Max-Age=99000";
</code></pre>

<p>Actually, after doing this, we have solved how to select &ldquo;on the fly&rdquo; the stack the user will use, now we can change the line to proxy the connection</p>

<pre><code>location / {
    proxy_pass http://$selected_upstream$request_uri;
}
</code></pre>

<p>We still need to have a way to balance the traffic between stacks. For that reason the default value of the map described above returns &ldquo;newuser&rdquo; when the cookie is not set.</p>

<ul>
<li><strong>Split Clients:</strong></li>
</ul>


<p>There are different ways of &ldquo;randomize&rdquo; the selected stack when a new user enters the site. We use a simple module of Nginx named <a href="http://nginx.org/en/docs/http/ngx_http_split_clients_module.html">split clients</a>, what it does basically is to hash an input string to a defined range of numbers and then check if the result of the hash is between a subrange in the predefined &ldquo;universe&rdquo; of the possible results. We use it because it is simple, but also using the perl module in Nginx to do calculation was considered; we don&rsquo;t need &ldquo;that much power&rdquo;.</p>

<p>As this gives always the same output and we wanted to get some more randomness, we send as the input string the local time (when the request was made), the IP and the User Agent. Only using IPs for some could solve the whole problem, but in our site a lot of users share the same IP and sometimes we want to deliver between them different versions of the site (hence, different stacks).</p>

<pre><code>split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version {
    1.0%    canary;
    *       fallback;
}
</code></pre>

<p>And we only need to use the output of the module (in the variable &ldquo;stack_version&rdquo;) to assign a first time user.</p>

<pre><code>if ($selected_upstream = "newuser") {
    set $selected_upstream $stack_version;
}
</code></pre>

<p>At this point we have an almost functional configuration, and the new users can be balanced between stacks by modifying the nginx configuration. For the logic we set up so far, moving users between stacks when they already have the cookie we need a flag to check what version of configuration they have.</p>

<pre><code>set $rollout_stage 1;
add_header Set-Cookie "site_rs=$rollout_stage;Domain=.test.me;Path=/;Max-Age=99000";
</code></pre>

<p>For moving users from the fallback stack (as we don&rsquo;t want to move the users from the canary, they already are in the new version) we can check if they belong to that stack and then again select a new one (probably).</p>

<pre><code>split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version_renew {
    1.0%    canary;
    *       fallback;
}

...

if ($cookie_site_rs != $rollout_stage) {
    set $candidate_split_clients "D";
}
if ($selected_upstream = "fallback"){
    set $candidate_split_clients "${candidate_split_clients}C";
}
if ($candidate_split_clients = "DC"){
    set $selected_upstream $stack_version_renew;
}
</code></pre>

<p>There are two peculiarities here, as Nginx is not capable of having &ldquo;OR&rdquo; or &ldquo;AND&rdquo; logic in the if statements, we need to check them separately (if the user is in &ldquo;fallback&rdquo; and if the user is in another rollout stage) and then check if both conditions are true.</p>

<p>The other is the need to use a separately split_modules to calculate the percentage of users which will move from &ldquo;fallback&rdquo; to &ldquo;canary&rdquo;. The percentage should be different (with the exceptions of 0% and 100%) for new users and existing ones.</p>

<p>Let&rsquo;s say we in the first stage want to deliver canary to 5% of the users. At this point all the users are new, so everybody will fall into the split for new users. Then, in the second stage of rollout we want to increase the percentage to 10%. For new users, setting 10% in the split is all that we need, but 95% of our existing users should be in the &ldquo;fallback&rdquo; stack. We only need to move 5.2631% of those users to canary in stage 2.</p>

<p>Talking with numbers (and some assumptions to understand this: 100 new users enter in each stage and all the previous users enter again); if in the first stage we have 100 users, 5 will go to &ldquo;canary&rdquo;, 95 will go to &ldquo;fallback&rdquo;. In the second stage, of the new users 10 will go to &ldquo;canary&rdquo; and 90 will go to &ldquo;fallback&rdquo;; for the old users of the first stage we only need to move 5 out of 95 from &ldquo;fallback&rdquo; to &ldquo;canary&rdquo;, that&rsquo;s or 5.2631% of fallback users.</p>

<table>
<thead>
<tr>
<th> Stage/Percentage  </th>
<th> New users &ldquo;C&rdquo;     </th>
<th> New users &ldquo;F&rdquo;     </th>
<th> Old users &ldquo;C&rdquo;         </th>
<th> Old users &ldquo;F&rdquo;     </th>
</tr>
</thead>
<tbody>
<tr>
<td> 1 - 5%            </td>
<td> 5                 </td>
<td> 95                </td>
<td>                       </td>
<td>                   </td>
</tr>
<tr>
<td> 2 - 10%           </td>
<td> 10                </td>
<td> 90                </td>
<td> 5 + 5 (5.2631%)       </td>
<td> 95 - 5            </td>
</tr>
<tr>
<td> 3 - 15%           </td>
<td> 15                </td>
<td> 85                </td>
<td> 20 + 10 (5.5555%)     </td>
<td> 180 - 10          </td>
</tr>
<tr>
<td> 4 - 20%           </td>
<td> 20                </td>
<td> 80                </td>
<td> 45 + 15 (5.8823%)     </td>
<td> 255 - 15          </td>
</tr>
<tr>
<td> 5 - 25%           </td>
<td> 25                </td>
<td> 75                </td>
<td> 80 + 20 (6.25%)       </td>
<td>                   </td>
</tr>
</tbody>
</table>


<p>The table shows us the following of the previous example. When we are in the third stage, we have 180 users in fallback and 20 in canary. We should have 170 in fallback and 30 in canary, so we need to move 10 users from fallback to canary, that&rsquo;s the 5.5555% of the 180 users.</p>

<p>In the fourth stage we have 255 users in fallback and 45 in canary, we should have 240 users in fallback and 60 in canary, so we need to move 15 users from fallback to canary, that&rsquo;s 5.8823% of the 255 users.</p>

<p>Finally, we have a configuration ready, which is:</p>

<pre><code>map $cookie_site_id $selected_upstream {
    default newuser;
    canary canary;
    fallback fallback;
}

split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version {
    1.0%    canary;
    *       fallback;
}

split_clients "${remote_addr}${http_user_agent}${time_local}" $stack_version_renew {
    1.0%    canary;
    *       fallback;
}

server {
    server_name                     m.test.me;
    listen 443 ssl;
    set $rollout_stage 1;
    if ($cookie_site_rs != $rollout_stage) {
        set $candidate_split_clients "D";
    }
    if ($selected_upstream = "fallback"){
        set $candidate_split_clients "${candidate_split_clients}C";
    }
    if ($candidate_split_clients = "DC"){
        set $selected_upstream $stack_version_renew;
    }
    if ($selected_upstream = "newuser") {
        set $selected_upstream $stack_version;
    }
    add_header Set-Cookie "site_id=$selected_upstream;Domain=.test.me;Path=/;Max-Age=99000";
    add_header Set-Cookie "site_rs=$rollout_stage;Domain=.test.me;Path=/;Max-Age=99000";

    location / {
        proxy_pass http://$selected_upstream$request_uri;
    }
}

upstream fallback {
    server          10.39.1.90 max_fails=5 fail_timeout=20;
    server          10.39.1.91 max_fails=5 fail_timeout=20;
}

upstream canary {
    server          10.39.0.93 max_fails=5 fail_timeout=20;
    server          10.39.0.92 max_fails=5 fail_timeout=20;
}
</code></pre>

<h2><strong>Expected Results</strong></h2>

<p>With the Continuous Delivery Implementation, we want to improve some important points in the company:</p>

<p><strong>Speed:</strong> We can deliver faster important improvements, fixes and features to our end users at any time, because we will be able to release some changes in hours or days. When we achieve this velocity our product may have fewer bugs and user experience will increase.</p>

<p><strong>Quality:</strong> This approach helps us learning faster about our errors so we can improve the thing we do and release products with more quality.</p>

<p><strong>Focus:</strong> All the team members can focus on finish and release a task before moving to another task. They don&rsquo;t need to switch their brains between tasks.</p>

<p><strong>Clarity:</strong> The team should experience less stress, because we have to fix small problems (if any) quickly so this conducts to a lower pressure work.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FreeIPA for Your Infrastructure Daily Operations]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/06/29/freeipa-control-access-and-okta/"/>
    <updated>2015-06-29T15:42:23-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/06/29/freeipa-control-access-and-okta</id>
    <content type="html"><![CDATA[<p>When you manage lots of servers (Unix and Linux like), dozens or hundreds of them, one thing for sure that you must take in account is the Access Control, and there are many many tools to help us to that specific task, but let&rsquo;s be realists!  In a minimal decent infrastructure implementation you don?t need to have only Access Control, you need as well policies depending on the user&rsquo;s profile. For example, you are not going to provide root permissions to a sales guy in a linux server for making queries to the main database or if you just want certain developers can execute a critical script (bash or python or whatever).</p>

<p>And we can say, come on! I have a tool that allows me to define Access Control as well as robust policies rules (<em>all mention before know as Identity and Policies</em>), for example LDAP or the well known MS Active Directory. But the Access Control is the only thing we have to take care about in our daily minimal server&rsquo;s operation?, obviously No! What about synchronization? this is to have all our server in the correct time for having accurate logs or naming resolution, if we want a friendly way to identify our servers, you know, things that are essentials in order to ensure that the basic communication and security its guaranteed.</p>

<p>Let&rsquo;s stop to talk about needs that aren?t new for us and let&rsquo;s talk about the solutions or in this case Segundamano&rsquo;s solution.</p>

<h2><strong>FreeIPA</strong></h2>

<p>We are not going to provide or copy/paste an explanation about what FreeIPA is from Wikipedia o Freeipa.org, in that case, please feel free to do it by yourself. We prefer to tell you in our experience what FreeIPA is doing for us. In Segundamano we use FreeIPA for the following approaches:</p>

<ul>
<li><strong>DNS Resolution</strong></li>
</ul>


<p>FreeIPA offers an integration of many open source components like Bind DNS Server, so we decided to use it as our main DNS Platform, In this way we can have multiple Bind slaves servers from it.</p>

<p>And why is this useful? Well It is useful because we want to have an instance for FreeIPA in each environment or Data Center in order to accelerate the DNS resolution and offer high availability.</p>

<p>Maybe you are wondering about what is a FreeIPA instance? Ok this topic will be explained on a next section on this document when we talk about replication which is a super nice feature.</p>

<ul>
<li><strong>DNS Master / Slave Deployment</strong></li>
</ul>


<p>The communication servers infrastructure in Segundamano is done thought DNS, we do not pretend to explain what a DNS service is, let&rsquo;s just say that if you only use for communication IP addressing and for any reason you have to change the database IP, you will have to change the IP in every single server that has communication with that servers and your memory factor is an issue, because you can forget a server that you never touch but it is crucial and yeap! Your platform becomes unstable or with abnormal behavior.</p>

<p>One of our main directives in Segundamano is to have (at least in production) High Availability in the main services that provides support to our platform. We are not going to enter in detail about what BCP and DRP are (Business Continuity Plan and Disaster Recovery Plan respectively), but as you may know we need to be prepared in case of a server failure or Data Center crash, that&rsquo;s why naming resolution service is critical to have it up &amp; running <em>7x24x365</em>.</p>

<p>So when we create a virtual machine (via kickstart) this is enrolled automatically to FreeIPA server and at the same time its DNS name is created (Record A and PTR). FreeIPA acts as the master DNS server and we create separate DNS Bind instances or servers (as you want to call them) which acts as the slaves and they receive the configuration from its master (FreeIPA) automatically.</p>

<p>Hey wait a minute, you have just said before in this article that this is a centralized architecture about Authentication, NTP, DNS Tool Integration! The answer is yes, in Segundamano we centralize these services in FreeIPA to do it only once and not hundreds of times, but we also believe in Distributed Systems and Automation, so we centralize many of our main services in FreeIPA and this &ldquo;guy&rdquo; distribute its own configuration to others in order to have High Availability, that is what becomes super cool FreeIPA.</p>

<p>Let&rsquo;s see the next diagram to have a better idea about DNS Master/Slave Replication:</p>

<p><img src="http://backstage.segundamano.mx/images/dns_master_slave.jpg" alt="DNS master/slave" /></p>

<ul>
<li><strong>Network Time Protocol (NTP) Synchronization</strong></li>
</ul>


<p>Talking about Network Time Protocol, we are using the one that FreeIPA includes, so maybe you are wondering about why the NTP protocol is very important? And the answer is that NTP allows to synchronize the date of all your infrastructure in order to have more accurate logs and processes (and some systems depend on time).</p>

<p>If you want to have a reliable and synchronized infrastructure maybe you have to consider pointing all your infrastructure to a NTP Server. And the main advantage that FreeIPA&rsquo;s NTP offers is the possibility to audit every system you manage, having accurate information about them.</p>

<ul>
<li><strong>User Management</strong></li>
</ul>


<p>We have a centralized user database, so we create a user only once in FreeIPA and we are able to log in across all different systems in the company. This have multiple benefits for us as systems administrators, such as:</p>

<p><strong>Modularity:</strong> FreeIPA allow us to create user and system groups. This give us the flexibility to create Access Policies at user or system level, going from general to specific. Also give us the ability to set up policies accordingly to the real roles in the company.</p>

<p><strong>Reuse of Policies:</strong> once we create a policy it can be applied to multiple scenarios, avoiding the manual replication and possible human errors by doing that.</p>

<ul>
<li><strong>Free IPA Master / Replica Deployment</strong></li>
</ul>


<p>This probably one of the most super cool features about FreeIPA: the capability to create as many FreeIPA instances as we need. Why do I have to create another replica? Well there are tons of reasons, we think that the main reason is&hellip; yes you guest right! High Availability.</p>

<p>At this point you maybe wondering based on the point where we discuss earlier in this article about DNS, if I can create many replicas, why to create Bind DNS instances if with the replicas should be enough? For sure you can use replicas to support with DNS, NTP, Authentication, etc. in a HA schema in a distributed way, that is fine if you want to do it that way. For Segundamano we want to have a Centralized Manager which gathers and command all the services mentioned and at the same time have independent services which receives the information they need from its master, in this case FreeIPA, because if, in some case we need to restart or reload a service, we only need to to do it in the service involved and not restart or reload the entire stack of FreeIPA services!</p>

<p>Going back to FreeIPA replica, let?s say that the hard part was to create from zero the first FreeIPA server, create a replica is super simple and after some minutes automagically you will have a clone of your master and could be promoted itself as a master (you can only have one master at the time), all replicas can be promoted as master whenever you need it, ain&rsquo;t that cool!</p>

<p>These are the steps to create a replica:</p>

<ol>
<li><p>In the master just enter the next line:</p>

<p> <em>ipa-replica-prepare your_ipa_server_name</em></p>

<p>it creates a file named replica-info-your_ipa_server_name</p></li>
<li><p> In the slave:</p>

<p><em>ipa-replica-install replica-info-your_ipa_server_name &ndash;setup-ca &ndash;setup-dns &ndash;no-forwarders &ndash;skip-conncheck</em></p>

<p><strong>&ndash;setup-ca.-</strong> put this parameter in order to promote your replica as master when you need it. This parameter is optional, but take into account that when you want to promote your replica, you will need to install the CA information at that moment.</p>

<p><strong>&ndash;setup-dns.-</strong> this parameter is to clone your DNS configuration to your replica as well as replicate DNS information.</p></li>
</ol>


<p>And that?s it!</p>

<p>In the next diagram we show you a brief overview about schema replication in Segundamano:</p>

<p><img src="http://backstage.segundamano.mx/images/freeipa_replication.jpg" alt="FreeIPA replication" /></p>

<h2><strong>Integration with OKTA</strong></h2>

<p>OKTA is a tool that once is integrated with an LDAP Directory allows us to have a centralized access and management of a predefined set of commercial applications such as Gmail, Google Apps, Slack, Github, Sales Force, etc. In Segundamano we had integrated FreeIPA as our LDAP Server with OKTA. After this we can login in all internal and external systems handled by OKTA with the same login information (user and password).</p>

<p>With this implementation we are able to manage our user database more efficiently and we can create policies which will rule the permissions to access the applications and systems based on the profiles of every person in the company.</p>

<p><strong>Advantages</strong></p>

<ul>
<li><p>A single access with the same password (SSO Single Sign ON)</p></li>
<li><p>Automatic Updates App of Web Apps (OKTA is responsible for updating all)</p></li>
<li><p>Integration of new tools (OKTA is continually developing new applications compatibility)</p></li>
</ul>


<p><img src="http://backstage.segundamano.mx/images/okta_apps.png" alt="OKTA apps" /></p>

<p><strong>Implementation</strong></p>

<p>Our experience implementing LDAP OKTA made us realize we must know beforehand the main parameters of our LDAP deployment</p>

<ul>
<li>orgUrl</li>
<li>ldaphost</li>
<li>LDAPPort</li>
<li>ldapAdminDN</li>
<li>ldapAdminPassword</li>
<li>baseDN</li>
</ul>


<p>It is essential to have a basic knowledge about LDAP (FreeIPA) for reliable deployment. Also we need to install an agent in FreeIPA server so you can have communication with OKTA (cloud), once the configuration is done in OKTA you have to log in at portal as an administrator.</p>

<p>The OKTA configuration is managed from FreeIPA (OKTA uses and agent to check everything is setup correctly), this is a simple way to describe the implementation of OKTA with LDAP.</p>

<p>Once the OKTA agent is configured in our LDAP it was necessary to enable part of SSO in Google Apps to access all our applications in Google, this will instruct Google to use the OKTA credentials instead of their own.</p>

<p>With this technique we avoid having several accesses and passwords for different tools.</p>

<p><img src="http://backstage.segundamano.mx/images/okta_architecture.png" alt="OKTA architecture" /></p>

<h2><strong>Expected Results</strong></h2>

<p>With this solution we should be able to manage easily and without waste of time the everyday applications by having a central point of control and giving the users a portal where they can access the apps.</p>

<p>This needs applications supporting OKTA integration, at the moment we&rsquo;re handling through this system the following apps:</p>

<ul>
<li>Google Aps</li>
<li>Gmail</li>
<li>Google Drive</li>
<li>Google Calendar</li>
<li>Slack</li>
<li>Yammer</li>
<li>Box</li>
<li>Github</li>
<li>Jira</li>
<li>Confluence</li>
<li>Sales Force (In process)</li>
</ul>


<p><img src="http://backstage.segundamano.mx/images/okta_supported_apps.png" alt="OKTA supported apps" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hackamano, Segundamano.mx's Own Hackathon Is Coming!]]></title>
    <link href="http://backstage.segundamano.mx/blog/2015/05/27/hackamano/"/>
    <updated>2015-05-27T08:26:26-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2015/05/27/hackamano</id>
    <content type="html"><![CDATA[<p>Segundamano.mx is organazing its very own Hackathon: Hackamano 2015.
The event will take place at the ITAM University campus in Mexico City
the 20 &amp; 21 June 2015.</p>

<p>The idea is that anyone can participate and be part of a team, with
a very ambitious goal in mind: develop solution to today&rsquo;s problems in
local communities.</p>

<p>This can span from improving access to education, water supply, waste
management, public transportation or whatever you think it makes sense
in your social context.</p>

<p>This will be the only rule, everything else will be limited by your own
imagination!</p>

<p>To get more information about the event, please refer to the <a href="http://hackamano.segundamano.mx">official page</a>. The registration page is also up and running <a href="http://bit.ly/hackamano-registro">here</a>. Places are limited, go register now!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Follow Us on GitHub!]]></title>
    <link href="http://backstage.segundamano.mx/blog/2014/10/21/follow-us-on-github/"/>
    <updated>2014-10-21T10:39:11-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2014/10/21/follow-us-on-github</id>
    <content type="html"><![CDATA[<p>Segundamano.mx is now on GitHub. Follow our <a href="https://github.com/SegundamanoMX">Organization Page</a>.</p>

<p>The first repository that we have publised is this blog itself, whose source code
will be available at <a href="https://github.com/SegundamanoMX/octopress">this location</a>
and we hope that with time we&rsquo;ll be able to publish more bits and pieces, including contributions
to third party FLOSS, our own software, and some challenges that we&rsquo;ll tackle together with some
local communities.</p>

<p>Stay tuned, and feel free to fork and contribute back on any of our public repositories!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgresql Metrics With Logstash]]></title>
    <link href="http://backstage.segundamano.mx/blog/2014/07/07/postgresql-metrics-pipeline/"/>
    <updated>2014-07-07T08:00:59-05:00</updated>
    <id>http://backstage.segundamano.mx/blog/2014/07/07/postgresql-metrics-pipeline</id>
    <content type="html"><![CDATA[<p>I thought I&rsquo;d share our setup at <a href="http://www.segundamano.mx">SegundaMano.mx</a>
for extracting PostgreSQL metrics with <a href="http://www.logstash.net">Logstash</a> to
push to graphs. We&rsquo;re planning on upgrading our database cluster from an older
PostgreSQL version, and we want to know what potential performance bottlenecks
we have before and after upgrading</p>

<h2>Goals</h2>

<p>Our goals here are to answer the questions:</p>

<ul>
<li>Which of our queries are slow?</li>
<li>What&rsquo;s the ratio of slow queries to non-slow queries?</li>
<li>What&rsquo;s the average query time?</li>
</ul>


<p>We want these in place before we do a major overhaul of our databases,
so we can see how our upgrades perform. Graphs are always better than
&ldquo;well it feels a lot better&rdquo;</p>

<p>Our choice of tooling landed on <a href="http://logstash.net">Logstash</a>
parsing the CSV log, extracting data to push to Statsd/Graphite and to
Elasticsearch/Kibana via RabbitMQ. In addition to Logstash,
other tools can extract data from the CSV log, for example
<a href="http://dalibo.github.io/pgbadger">pgBadger</a> and PostgreSQL
itself.</p>

<p>From the CSV log, we extract the query duration of all queries and
push this to statsd. We also fire off a counter for each query, and a
separate for each slow query.</p>

<h2>Parts involved</h2>

<p>We broke this down into 4 different roles - this might change at some
later point but it&rsquo;s a start. I drew a little picture with the various
components - the dotted lines denote host boundaries</p>

<p><img src="http://backstage.segundamano.mx/images/logstash_pipeline.png"></p>

<h3>Database servers</h3>

<ul>
<li>Postgres with CSV logging</li>
<li>Logstash</li>
<li>Statsd</li>
</ul>


<h3>RabbitMQ servers</h3>

<ul>
<li>RabbitMQ in a cluster</li>
</ul>


<h3>Graphite server</h3>

<ul>
<li>Graphite (carbon-cache, whisper, graphite-web)</li>
</ul>


<h3>Elasticsearch server:</h3>

<ul>
<li>Logstash indexer</li>
<li>Elasticsearch</li>
<li>Kibana</li>
<li>Nginx</li>
</ul>


<h2>The pipeline components</h2>

<h2>Logstash</h2>

<p><a href="http://logstash.net">Logstash</a> is basically a Unix pipe on steroids.
It&rsquo;s a JRuby project with a lot of input, filter, and output plugins
hosted by Elasticsearch. It really shines when connected with
ElasticSearch and Kibana.</p>

<p>Usually what&rsquo;s done is a logstash &ldquo;agent&rdquo; instance runs on each server
that parses the logs, mangles them into your correct format, and push
them to a central broker. Then an &ldquo;indexer&rdquo; instance pulls them off
the broker and indexes them into ElasticSearch. This decouples the
instances a bit, allowing you to firewall off the ES instance from the
servers, or use a lightweight shipper agent lacking ES support (such
as <a href="http://beaver.readthedocs.org/en/latest/">Beaver</a>) to ship
entries.</p>

<p>As for brokers, the most common are Redis and RabbitMQ. The easiest to
use is Redis, using a queue or a channel. Unfortunately this doesn&rsquo;t
offer the routing possibilites of RabbitMQ, or the ability to see
events coming over in real time with a script performing <code>tail -f</code>
duties - with Redis you can see the firehose with <code>MONITOR</code> and that&rsquo;s it.</p>

<h2>RabbitMQ</h2>

<p><a href="http://www.rabbitmq.org">RabbitMQ</a> is an open source AMQP broker. We
have a separate Vhost for logstash, and separate users for indexer and
publisher.</p>

<p>In the vhost there&rsquo;s a persistent <code>topic</code> exchange. This means that we
can shut down either the producer or consumer without losing events on
the floor - they get queued up until the indexer comes alive again.
For availability we run a cluster over several hosts using a virtual
IP with keepalived for the clients to connect to.</p>

<h2>Statsd</h2>

<p><a href="https://github.com/etsy/statsd/">Statsd</a> is a project from Etsy. You
shove metrics to it (types available
<a href="https://github.com/etsy/statsd/blob/master/docs/metric_types.md">here</a>),
and it handles the per-timeunit bucketing, mean/std/upper90 of timers
et c and ships data to Graphite. Logstash has this as well in the
<a href="http://logstash.net/docs/1.4.1/filters/metrics">metrics</a> filter, but
it doesn&rsquo;t behave as well as I&rsquo;d like.</p>

<p>We use Vimeo&rsquo;s
<a href="https://github.com/vimeo/statsdaemon">go implementation</a> instead of
Etsy&rsquo;s NodeJS version. The main reasoning being go projects compile to
a static binary, simplifying deploys (since we don&rsquo;t use NodeJS in
production). I built a
<a href="https://gist.github.com/lflux/934eff42924e276c8673">RPM specfile</a> to
build an RPM of this.</p>

<h2>Graphite</h2>

<p><a href="http://graphite.readthedocs.org/">Graphite</a> is a graphing system
comprising several parts. Carbon-cache receives the metrics and stores
them in Whisper. Whisper is a timeseries database that doesn&rsquo;t lose resolution,
like RRD does. Graphite-web is a Django project that handles the user
interface and rendering.</p>

<p>We use PostgreSQL as the storage backend instead of
sqlite after reading <a href="http://obfuscurity.com/2013/12/Why-You-Shouldnt-use-SQLite-with-Graphite">this blog post</a></p>

<h2>Elasticsearch / kibana</h2>

<p>Logstash publishes the events into
<a href="http://www.elasticsearch.org">ElasticSearch</a>, with one index per
day. <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a> is a
HTML/JS frontend to Elasticsearch for viewing the log data. The nice
thing about Kibana is that it&rsquo;s really easy to search in the data and
randomly play around with different queries - it starts off with all
events and has nice breakdowns of the values of each. At my home
company, we use Kibana to make sense of the errors coming off our app
servers and have caught a lot of interesting bugs from randomly
playing around with the queries.</p>

<h2>Results</h2>

<p>A few hours after we enabled the whole pipeline, we already could use
the Kibana interface to spot slow queries, specifically two major
offenders that we could clear up with one index on a table.</p>

<p>Kibana slow query count:
<img src="http://backstage.segundamano.mx/images/kibana_slows.png"></p>

<p>Graphite queries vs slow queries
<img src="http://backstage.segundamano.mx/images/query_count.png"></p>

<p>Graphite query time
<img src="http://backstage.segundamano.mx/images/query_time.png"></p>

<!-- more -->


<h2>Configuration Details</h2>

<h2>On the PostgreSQL server</h2>

<h3>postgresql.conf</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Log both to syslog and CSV. A .log file will be created, but it will
</span><span class='line'>be empty.
</span><span class='line'>log_destination = 'syslog,csvlog'
</span><span class='line'>log_filename = 'postgresql-%Y-%m-%d.log'
</span><span class='line'># Log duration of all statements. 
</span><span class='line'># Log full statement of any that takes  more than 2 seconds.
</span><span class='line'>log_duration = on
</span><span class='line'>log_min_duration_statement = 2000
</span><span class='line'>log_statement = 'none'
</span><span class='line'># If you log in a locale such as es_MX, logstash might not parse the CSV correctly
</span><span class='line'>lc_messages = 'C'</span></code></pre></td></tr></table></div></figure>


<h3>Logstash shipper</h3>

<figure class='code'><figcaption><span>logstash-postgresql-shipper.conf</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>        file {
</span><span class='line'>                "path" =&gt; "/path/to/pg_log/*.csv"
</span><span class='line'>                "sincedb_path" =&gt; "/path/to/pg_log/sincedb_pgsql"
</span><span class='line'># fix up multiple lines in log output into one entry              
</span><span class='line'>           codec =&gt; multiline {
</span><span class='line'>                   pattern =&gt; "^%{TIMESTAMP_ISO8601}.*"
</span><span class='line'>                   what =&gt; previous
</span><span class='line'>                   negate =&gt; true
</span><span class='line'>           }
</span><span class='line'>        }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'># See http://www.postgresql.org/docs/9.3/interactive/runtime-config-logging.html#RUNTIME-CONFIG-LOGGING-CSVLOG
</span><span class='line'>        csv {
</span><span class='line'> columns =&gt; [  "log_time", "user_name", "database_name", "process_id", "connection_from", "session_id", "session_line_num", "command_tag", "session_start_time", "virtual_transaction_id", "transaction_id", "error_severity", "sql_state_code", "sql_message", "detail", "hint", "internal_query", "internal_query_pos", "context", "query", "query_pos", "location"]
</span><span class='line'>        }
</span><span class='line'>  mutate {
</span><span class='line'>         gsub =&gt; [ "sql_message", "[\n\t]+", " "]
</span><span class='line'>  }
</span><span class='line'># use timestamp from log file  
</span><span class='line'>  date {
</span><span class='line'>        #2014-05-22 17:02:35.069 CDT
</span><span class='line'>        match =&gt; ["log_time", "YYYY-MM-dd HH:mm:ss.SSS z"]
</span><span class='line'>}
</span><span class='line'>  grok {
</span><span class='line'>    match =&gt; ["sql_message", "duration: %{DATA:duration:int} ms"]
</span><span class='line'>    tag_on_failure =&gt; []
</span><span class='line'>        add_tag =&gt; "sql_message"
</span><span class='line'>  }
</span><span class='line'># See postgres configuration - a message with 'statement: ' is a slow query
</span><span class='line'>  grok  {
</span><span class='line'>        match =&gt;["sql_message", "statement: %{GREEDYDATA:statement}"]
</span><span class='line'>        tag_on_failure =&gt; []
</span><span class='line'>        add_tag =&gt; "slow_statement"
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'># Increase hitcounter when we see a slow statement
</span><span class='line'>  if "slow_statement" in [tags] {
</span><span class='line'>        statsd {
</span><span class='line'>                increment =&gt; "postgresql.slow_queries"
</span><span class='line'>        }
</span><span class='line'>  }
</span><span class='line'>  # increase hitcounter for all queries
</span><span class='line'>  # send timing metrics for queries
</span><span class='line'>  if "sql_message" in [tags] {
</span><span class='line'>          statsd {
</span><span class='line'>                timing =&gt; {"query_duration" =&gt; "%{duration}"}
</span><span class='line'>                increment =&gt; "postgresql.queries"
</span><span class='line'>          }
</span><span class='line'>  }
</span><span class='line'>  # Ship off all to rabbitmq
</span><span class='line'>  rabbitmq {
</span><span class='line'>    'durable' =&gt; true
</span><span class='line'>    'exchange' =&gt; 'logstash'
</span><span class='line'>    'exchange_type' =&gt; 'topic'
</span><span class='line'>    'host' =&gt;  "server"
</span><span class='line'>  #note - replace type and host manually since as of writing this isn't replaced like it should be
</span><span class='line'>    'key' =&gt; 'logstash.%{type}.%{host}'
</span><span class='line'>    'password' =&gt; "password"
</span><span class='line'>    'persistent' =&gt; true
</span><span class='line'>    'user' =&gt; 'shipper'
</span><span class='line'>    'vhost' =&gt; 'logstash'
</span><span class='line'>    'workers' =&gt; 4
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>Statsdaemon</h3>

<p>Not much to this here - just run statsdaemon and edit the configuration file <code>/etc/statsdaemon.ini</code> and set the correct graphite host.</p>

<h2>RabbitMQ</h2>

<p>Set up rabbitmq with a Vhost for logstash, create users indexer and
shipper that can read and write to the logstash vhost. Before starting
the shipper, start the indexer which will automatically create the
binding from the exchange to the queue.</p>

<h2>ElasticSearch/Indexers</h2>

<h3>ElasticSearch</h3>

<p>Install elasticsearch with a fair bit of storage.</p>

<figure class='code'><figcaption><span>/etc/elasticsearch/elasticsearch.yml</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">cluster.name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">elasticsearch</span>
</span><span class='line'><span class="l-Scalar-Plain">path.conf</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/etc/elasticsearch</span>
</span><span class='line'><span class="l-Scalar-Plain">path.data</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/opt/logs/elasticsearch/data</span>
</span><span class='line'><span class="l-Scalar-Plain">path.logs</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/opt/logs/elasticsearch/logs</span>
</span><span class='line'><span class="l-Scalar-Plain">discovery.zen.minimum_master_nodes</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">1</span>
</span><span class='line'><span class="l-Scalar-Plain">discovery.zen.ping.multicast.enabled</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Logstash Indexer</h3>

<p>Configure to pull events off RabbitMQ and index them into Elasticsearch</p>

<figure class='code'><figcaption><span>/opt/logstash/server/etc/conf.d/logstash.conf</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>
</span><span class='line'>      rabbitmq {
</span><span class='line'>        'auto_delete' =&gt; 'false'
</span><span class='line'>        'codec' =&gt; 'json'
</span><span class='line'>        'debug' =&gt; true
</span><span class='line'>        'durable' =&gt; true
</span><span class='line'>        'exchange' =&gt; 'logstash'
</span><span class='line'>        'exclusive' =&gt; false
</span><span class='line'>        'host' =&gt; 'host'
</span><span class='line'>        'key' =&gt; 'logstash.#'
</span><span class='line'>        'password' =&gt; 'passwd'
</span><span class='line'>        'queue' =&gt; 'logstash-indexer'
</span><span class='line'>        'user' =&gt; 'indexer'
</span><span class='line'>        'vhost' =&gt; 'logstash'
</span><span class='line'>      }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch { host =&gt; "host"
</span><span class='line'>        cluster =&gt; "logstash"
</span><span class='line'>        protocol =&gt; "http"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>Kibana</h3>

<p>Install kibana from <a href="https://github.com/elasticsearch/kibana">git master</a> into the directory
where you want it served from.</p>

<h3>Nginx</h3>

<p>Set up Nginx to show kibana HTML and to proxy requests to Elasticsearch</p>

<figure class='code'><figcaption><span>/etc/nginx/sites-enabled/kibana</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class='nginx'><span class='line'><span class="k">server</span> <span class="p">{</span>
</span><span class='line'>  <span class="kn">listen</span>                <span class="n">host</span><span class="p">:</span><span class="mi">80</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="kn">server_name</span>           <span class="s">host</span><span class="p">;</span>
</span><span class='line'>  <span class="kn">access_log</span>            <span class="s">/var/log/nginx/kibana.log</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="kn">location</span> <span class="s">/</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">root</span>  <span class="s">/opt/kibana/current/src</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">index</span>  <span class="s">index.html</span>  <span class="s">index.htm</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_aliases$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_aliases$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_nodes$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_search$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/.*/_mapping$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/_cluster/health$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># password protected end points</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/kibana-int/dashboard/.*$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kn">location</span> <span class="p">~</span> <span class="sr">^/kibana-int/temp.*$</span> <span class="p">{</span>
</span><span class='line'>    <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:9200</span><span class="p">;</span>
</span><span class='line'>    <span class="kn">proxy_read_timeout</span> <span class="mi">90</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
</feed>
